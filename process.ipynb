{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective of the project üöÄ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My objective is to create a machine learning model for object classification and then, try and compare it with a convolutional neural network (CNN) for images. For that, I will use Spark MLlib to train and evaluate the model. Secondly, create the CNN and compare both ü™ê\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source of the data is: https://skyserver.sdss.org/CasJobs/\n",
    "It is necessary to register and login a user to download the data you want. Then, you have to make a query specifying:\n",
    "- Amount of rows\n",
    "- Columns\n",
    "- Where to keep the csv\n",
    "- The database\n",
    "\n",
    "As I want to get as much as possible data, I will not a maximum of rows.\n",
    "\n",
    "I also add a \"where\" so I can get only data from planets, galaxies and stars:\n",
    "- type = 3: Galaxies\n",
    "- type = 6: Stars\n",
    "\n",
    "and I downloaded the dataset to start working with it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/home/haizeagonzalez/myproject/bigDataAstronomy/notebookImages/img1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns we have are:\n",
    "- objID: Unique identifier of the object ‚Üí TYPE bigInt\n",
    "- ra: Right ascension ‚Üí TYPE float\n",
    "- dec: Declination ‚Üí TYPE float\n",
    "- petroRad: Petrosian radius, used to know the size of galaxies in astronomical pictures. It is the amount of light that a galaxy emits in a sepecific radius. Very used because it is independent of the distance and brightness. We use different photometric filters:\n",
    "    - petroRad_u: Near-ultraviolet\n",
    "    - petroRad_g: Blue-Green\n",
    "    - petroRad_r: Red\n",
    "    - petroRad_i: Near-infrared\n",
    "    - petroRad_z: Deeper infrared\n",
    " ‚Üí TYPE: Real\n",
    "\n",
    "- modelMag: Brightness measure adjusted to a galaxy model. Usual for galaxies. Also for all filters (u, g, r, i and z) ‚Üí TYPE Real\n",
    "- psfMag: Brightness measure based on the point source light profile. Usual for stars. Also for all filters (u, g, r, i and z) ‚Üí TYPE Real\n",
    "- u_g: (modelMag_u - modelMag_g)\n",
    "- g_r: (modelMag_g - modelMag_r)\n",
    "- r_i: (modelMag_r - modelMag_i)\n",
    "- i_z: (modelMag_i - modelMag_z)\n",
    "- fracDeV: The amount of brightness that the object has in the De Vaucouleurs profile. Also for all filters (u, g, r, i and z) ‚Üí TYPE Real\n",
    "- flags: Bit comination that explains different characteristics of the object. If we convert it to binary and check SDSS documentarion, we get a meaning for each bit ‚Üí TYPE bigInt\n",
    "- clean: Indicator that tell us if the object was cleaned ‚Üí TYPE int\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PetroRad:\n",
    "- Stars: Small and constant in all filters.\n",
    "- Galaxies: Bigger and variates depending on the wavelengths.\n",
    "\n",
    "ModelMag and psfMag:\n",
    "- In the red filter:\n",
    "    - Stars: modelMag_r ‚âà psfMag_r\n",
    "    - Galaxies: modelMag_r > psfMag_r\n",
    "- In other filers:\n",
    "    - Galaxies are usuarlly more red  (modelMag_g - modelMag_r is big).\n",
    "    - Stars has different colors depending on their type.\n",
    "\n",
    "fracDeV:\n",
    "- Stars: fracDeV ‚âà 0.\n",
    "- Galaxies: fracDeV ‚âà 1 (eliptic) or fracDeV < 1 (espiral).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a spark sesion in case there is no one or get if there exists: \"getOrCreate\". I also decided to create a log in case there is any error during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/03/16 17:45:09 WARN Utils: Your hostname, SS22-3006967600 resolves to a loopback address: 127.0.1.1; using 172.20.232.160 instead (on interface eth0)\n",
      "25/03/16 17:45:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/16 17:45:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"bigDataAstronomyProject\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to read de csv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/haizeagonzalez/bigDataProject/primaryObjs.csv\"\n",
    "path2 = \"/home/haizeagonzalez/myproject/primaryObjs_reduced.csv\"\n",
    "\n",
    "df = spark.read.csv(path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to check if the data is correctly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+------------------+----+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+--------+--------+--------+--------+---------+---------+-------------+----------+---------+---------+---------+---------+----------+---------------+-----+\n",
      "|              objID|              ra|               dec|type|petroRad_u|petroRad_g|petroRad_r|petroRad_i|petroRad_z|modelMag_u|modelMag_g|modelMag_r|modelMag_i|modelMag_z|psfMag_u|psfMag_g|psfMag_r|psfMag_i|psfMag_z|      u_g|      g_r|          r_i|       i_z|fracDeV_u|fracDeV_g|fracDeV_r|fracDeV_i| fracDeV_z|          flags|clean|\n",
      "+-------------------+----------------+------------------+----+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+--------+--------+--------+--------+---------+---------+-------------+----------+---------+---------+---------+---------+----------+---------------+-----+\n",
      "|1237648704596673591|228.133205738866| 0.189419197306071|   3|  2.969285|  2.969369|  2.220518|  1.249783|   1.62232|  23.71841|  23.55119|  21.70663|  21.07082|  20.94918|23.71531|24.25853|22.41145|21.53375|21.45539|0.1672249| 1.844557|    0.6358051| 0.1216412|        1|        0|        1|        1|         1|316730484195600|    1|\n",
      "|1237648704596673595|228.132667119533| 0.155016320524743|   3|  2.969285|  1.167545|    1.7132|  4.193829|  1.581682|  26.23039|  23.06768|  21.60064|  21.04392|  20.49498|25.81252|23.30065|22.16619|21.59607|20.93345|  3.16271| 1.467041|    0.5567226|  0.548933|        0|        1|0.5398329|        1|         0|    68987912448|    1|\n",
      "|1237648704596673596|228.133591481833|0.0626739181110149|   6|  7.357505|  1.317162|  1.048602|  0.980907| 0.9447467|   23.2214|  21.66433|  20.29408|  19.12956|  18.53254|23.29687|21.69403|20.32223|19.14626|18.55519| 1.557074| 1.370243|     1.164522| 0.5970211|        1|        1|        1|        0|         0|    69055021384|    1|\n",
      "|1237648704596673597|228.134526612738| 0.184396878919884|   3|  4.583073|  1.801946|  1.712365|  1.611359|  1.693891|  24.13848|   22.0195|  21.33374|  21.02932|  21.22839|24.35229| 22.5369|21.95571|21.76257|21.68405| 2.118977|0.6857586|    0.3044224|-0.1990643|0.2213039|        0|        0|        0|         0|    68987912448|    1|\n",
      "|1237648704596673600|228.134746034796|0.0513303225187829|   3|  7.357505|  1.275892|  1.897987|  2.219621|  2.806328|  25.94563|  22.67289|  20.76974|  20.01449|   19.5253|25.25427|23.28042|21.51431|20.77017|20.30231| 3.272739| 1.903149|    0.7552528|  0.489193|        1|        0|0.8176387|        1|0.05972257|    68987912448|    1|\n",
      "|1237648704596673601|228.135436960298|0.0465206752009805|   3|  1.529451|  1.516517|  1.452753|  3.800702|  2.970232|  22.62748|  22.42105|  21.79902|  21.57086|  22.21361|22.84985|22.74921|22.23782|21.97033|22.15221|0.2064247|0.6220398|    0.2281551|-0.6427536|        1|        1|        0|        1|         1|    68987912448|    1|\n",
      "|1237648704596673602|228.135810120824|0.0383140532919898|   3|  1.823351|  2.052975|  1.442957|  1.195688|  1.241435|  23.31253|  22.18809|  21.72752|    21.728|  21.49295|23.47264|22.53271|22.08589|    22.1|21.72621| 1.124439|0.4605713|-0.0004806519| 0.2350483|        1|0.7402861| 0.160501|0.3372153| 0.7511653|    68987912192|    1|\n",
      "|1237648704596673603|228.135706969247| 0.135016964266971|   6| 0.7134073|  1.140595|  1.132749| 0.9277821|  0.931531|  24.37991|  22.96101|  21.76162|  20.41283|  19.70002|24.20988|22.96505|21.82184|20.45126|19.70096| 1.418892| 1.199396|     1.348785| 0.7128162|        0|        1|        0|        1|         0|    68987912192|    1|\n",
      "|1237648704596673605|228.137119120399| 0.138930706793513|   3|  2.969285|  2.969369|  2.519946|  2.969986|  7.359852|  23.42524|  22.22177|  21.80001|  22.03525|  23.73267|24.56458|23.02275|22.87213|23.26385|23.09968| 1.203466|0.4217625|    -0.235239| -1.697422|        0|        0|        0|        0|         1|281543964623104|    1|\n",
      "|1237648704596673613|228.138360776843|0.0731866066746134|   3|  1.046744|   1.88157|  1.307295| 0.9633411|  1.562622|  23.02948|  22.25664|  21.87386|  21.54906|  21.34355|22.90016|22.49634|22.23204|21.75176|21.65852|0.7728405|0.3827801|    0.3247986| 0.2055111|        1|        1|        0|        1|         0|105555532185616|    1|\n",
      "|1237648704596673615|228.138386598436| 0.187229164179413|   6|  1.083686|  1.201956| 0.9640225| 0.9422169|  1.116567|  23.75787|  22.49124|   21.1148|  20.39421|  19.97207|23.75588|22.50238|21.15819|20.42044| 20.0078| 1.266636| 1.376434|    0.7205887|  0.422142|        1|        1|0.9983205|        0|         0|    68987912192|    1|\n",
      "|1237648704596673616|228.138678791242|0.0125585461003744|   3|  2.905516|   2.37311|   2.34492|  2.162898|  2.206184|  22.42896|  21.56379|  20.59733|  19.86494|  19.63614|23.12312|22.40815|21.56322| 20.8563|20.60041|0.8651695|0.9664516|    0.7323971| 0.2287998|        1|        0|        0|        0|         0|  2450547277824|    1|\n",
      "|1237648704596673618|228.139331703426| 0.212781082931336|   6|  2.969285|   1.09076|   1.08924| 0.9051506|  1.057727|  25.92635|  23.35114|  21.78671|  20.63976|  19.91965|25.89036|23.30975|21.77252|20.63534|19.91474| 2.575216| 1.564421|     1.146955| 0.7201118|        0|        1|        0|        0|         0|    68987912448|    1|\n",
      "|1237648704596673620| 228.14091295114|0.0641572455788585|   6|  2.969285|  1.131206|  1.105575|  1.009262|  1.011155|  25.28518|  21.63824|  20.31469|  19.49671|  19.01631|25.20087|21.63558|20.32779|19.49734|19.02662| 3.646946| 1.323544|    0.8179798|  0.480402|        1|        0| 0.511569|        0|         1| 35253360001296|    1|\n",
      "|1237648704596673626| 228.14182144385|0.0425139072777563|   3| 0.9543655|  2.969369|  2.042264|  1.426649|  1.640048|  23.36676|  22.47925|  21.09399|  20.61784|  20.24213|23.61166|23.02375|21.73804|21.21591|20.80515|0.8875103| 1.385258|    0.4761486| 0.3757133|        1|        1|        1|0.6541844| 0.4127298| 17661174087936|    1|\n",
      "|1237648704596673630|228.142630123835|0.0980218578449677|   6|   27.9153|  1.133189|  2.300974|   2.93181|  1.672956|  25.39514|   23.0061|  21.70307|  20.86877|  20.44747|25.06125|22.99777|21.73359|20.86797|20.44591|  2.38904| 1.303038|    0.8342972| 0.4212971|        0|        0|        0|        1|         1| 35253360136976|    1|\n",
      "|1237648704596673633|228.142664146071| 0.197280424562705|   3|  2.969285|  2.969369|  1.437083|  1.543922| 0.8909767|  23.27159|  22.77104|  21.70474|  21.28639|  20.96748| 23.3143|23.06283|22.08174|21.67632|21.29585|0.5005493| 1.066303|    0.4183445| 0.3189125|        1|        1|        0| 0.286461|         1|105622104178960|    1|\n",
      "|1237648704596673636|228.143752563171|0.0624049229984539|   3|  0.691259|  2.969369|  1.262619|  1.890306|  2.575164|  23.24219|  22.31861|  21.90146|  21.72495|  21.07799|23.17297|22.68129|22.31081| 22.2052|21.41852|0.9235802|0.4171486|    0.1765156| 0.6469593|        1|        0|        0|        0|         0|  2416187540224|    1|\n",
      "|1237648704596673637|228.144495675809| 0.111100316350891|   6| 0.8216941| 0.9715834| 0.9761356| 0.9204842| 0.9451327|  24.14613|   22.8366|   21.5246|  20.31857|  19.64246|24.20292|22.84755|21.48754|20.30396|19.65173| 1.309528| 1.312006|     1.206022| 0.6761169|        0|        1|        0|        1|         1|    68987912192|    1|\n",
      "|1237648704596673639|228.144350348705|0.0484642813853641|   3|  1.962816|  1.365865|  1.481539|  1.819084|  1.651328|  22.93299|   22.2439|  21.15378|  20.66732|   20.4084|23.29018|22.57443| 21.5526|21.11196|21.01716|0.6890869| 1.090128|    0.4864521| 0.2589226|        0|        0|        0|0.3870666|         0| 35253360001040|    1|\n",
      "+-------------------+----------------+------------------+----+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+--------+--------+--------+--------+---------+---------+-------------+----------+---------+---------+---------+---------+----------+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema and the chacacteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- objID: string (nullable = true)\n",
      " |-- ra: string (nullable = true)\n",
      " |-- dec: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- petroRad_u: string (nullable = true)\n",
      " |-- petroRad_g: string (nullable = true)\n",
      " |-- petroRad_r: string (nullable = true)\n",
      " |-- petroRad_i: string (nullable = true)\n",
      " |-- petroRad_z: string (nullable = true)\n",
      " |-- modelMag_u: string (nullable = true)\n",
      " |-- modelMag_g: string (nullable = true)\n",
      " |-- modelMag_r: string (nullable = true)\n",
      " |-- modelMag_i: string (nullable = true)\n",
      " |-- modelMag_z: string (nullable = true)\n",
      " |-- psfMag_u: string (nullable = true)\n",
      " |-- psfMag_g: string (nullable = true)\n",
      " |-- psfMag_r: string (nullable = true)\n",
      " |-- psfMag_i: string (nullable = true)\n",
      " |-- psfMag_z: string (nullable = true)\n",
      " |-- u_g: string (nullable = true)\n",
      " |-- g_r: string (nullable = true)\n",
      " |-- r_i: string (nullable = true)\n",
      " |-- i_z: string (nullable = true)\n",
      " |-- fracDeV_u: string (nullable = true)\n",
      " |-- fracDeV_g: string (nullable = true)\n",
      " |-- fracDeV_r: string (nullable = true)\n",
      " |-- fracDeV_i: string (nullable = true)\n",
      " |-- fracDeV_z: string (nullable = true)\n",
      " |-- flags: string (nullable = true)\n",
      " |-- clean: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all columns are string, we need to convert them into their type. For that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- objID: long (nullable = true)\n",
      " |-- ra: float (nullable = true)\n",
      " |-- dec: float (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- petroRad_u: float (nullable = true)\n",
      " |-- petroRad_g: float (nullable = true)\n",
      " |-- petroRad_r: float (nullable = true)\n",
      " |-- petroRad_i: float (nullable = true)\n",
      " |-- petroRad_z: float (nullable = true)\n",
      " |-- modelMag_u: float (nullable = true)\n",
      " |-- modelMag_g: float (nullable = true)\n",
      " |-- modelMag_r: float (nullable = true)\n",
      " |-- modelMag_i: float (nullable = true)\n",
      " |-- modelMag_z: float (nullable = true)\n",
      " |-- psfMag_u: float (nullable = true)\n",
      " |-- psfMag_g: float (nullable = true)\n",
      " |-- psfMag_r: float (nullable = true)\n",
      " |-- psfMag_i: float (nullable = true)\n",
      " |-- psfMag_z: float (nullable = true)\n",
      " |-- u_g: float (nullable = true)\n",
      " |-- g_r: float (nullable = true)\n",
      " |-- r_i: float (nullable = true)\n",
      " |-- i_z: float (nullable = true)\n",
      " |-- fracDeV_u: float (nullable = true)\n",
      " |-- fracDeV_g: float (nullable = true)\n",
      " |-- fracDeV_r: float (nullable = true)\n",
      " |-- fracDeV_i: float (nullable = true)\n",
      " |-- fracDeV_z: float (nullable = true)\n",
      " |-- flags: long (nullable = true)\n",
      " |-- clean: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"objID\", col(\"objID\").cast(\"long\")) \\\n",
    "       .withColumn(\"ra\", col(\"ra\").cast(\"float\")) \\\n",
    "       .withColumn(\"dec\", col(\"dec\").cast(\"float\")) \\\n",
    "       .withColumn(\"petroRad_u\", col(\"petroRad_u\").cast(\"float\")) \\\n",
    "       .withColumn(\"petroRad_g\", col(\"petroRad_g\").cast(\"float\")) \\\n",
    "       .withColumn(\"petroRad_r\", col(\"petroRad_r\").cast(\"float\")) \\\n",
    "       .withColumn(\"petroRad_i\", col(\"petroRad_i\").cast(\"float\")) \\\n",
    "       .withColumn(\"petroRad_z\", col(\"petroRad_z\").cast(\"float\")) \\\n",
    "       .withColumn(\"modelMag_u\", col(\"modelMag_u\").cast(\"float\")) \\\n",
    "       .withColumn(\"modelMag_g\", col(\"modelMag_g\").cast(\"float\")) \\\n",
    "       .withColumn(\"modelMag_r\", col(\"modelMag_r\").cast(\"float\")) \\\n",
    "       .withColumn(\"modelMag_i\", col(\"modelMag_i\").cast(\"float\")) \\\n",
    "       .withColumn(\"modelMag_z\", col(\"modelMag_z\").cast(\"float\")) \\\n",
    "       .withColumn(\"psfMag_u\", col(\"psfMag_u\").cast(\"float\")) \\\n",
    "       .withColumn(\"psfMag_g\", col(\"psfMag_g\").cast(\"float\")) \\\n",
    "       .withColumn(\"psfMag_r\", col(\"psfMag_r\").cast(\"float\")) \\\n",
    "       .withColumn(\"psfMag_i\", col(\"psfMag_i\").cast(\"float\")) \\\n",
    "       .withColumn(\"psfMag_z\", col(\"psfMag_z\").cast(\"float\")) \\\n",
    "       .withColumn(\"u_g\", col(\"u_g\").cast(\"float\")) \\\n",
    "       .withColumn(\"g_r\", col(\"g_r\").cast(\"float\")) \\\n",
    "       .withColumn(\"r_i\", col(\"r_i\").cast(\"float\")) \\\n",
    "       .withColumn(\"i_z\", col(\"i_z\").cast(\"float\")) \\\n",
    "       .withColumn(\"fracDeV_u\", col(\"fracDeV_u\").cast(\"float\")) \\\n",
    "       .withColumn(\"fracDeV_g\", col(\"fracDeV_g\").cast(\"float\")) \\\n",
    "       .withColumn(\"fracDeV_r\", col(\"fracDeV_r\").cast(\"float\")) \\\n",
    "       .withColumn(\"fracDeV_i\", col(\"fracDeV_i\").cast(\"float\")) \\\n",
    "       .withColumn(\"fracDeV_z\", col(\"fracDeV_z\").cast(\"float\")) \\\n",
    "       .withColumn(\"flags\", col(\"flags\").cast(\"long\")) \\\n",
    "       .withColumn(\"clean\", col(\"clean\").cast(\"int\"))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the structure, we are going to explore and clean the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, the data is cleaned because we get it from CasJobs and we apply clear filter to get good data. However, we are going to check whether there is any null value and the amount of galaxies and stars.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+----+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+--------+--------+--------+--------+---+---+---+---+---------+---------+---------+---------+---------+-----+-----+\n",
      "|objID| ra|dec|type|petroRad_u|petroRad_g|petroRad_r|petroRad_i|petroRad_z|modelMag_u|modelMag_g|modelMag_r|modelMag_i|modelMag_z|psfMag_u|psfMag_g|psfMag_r|psfMag_i|psfMag_z|u_g|g_r|r_i|i_z|fracDeV_u|fracDeV_g|fracDeV_r|fracDeV_i|fracDeV_z|flags|clean|\n",
      "+-----+---+---+----+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+--------+--------+--------+--------+---+---+---+---+---------+---------+---------+---------+---------+-----+-----+\n",
      "|    0|  0|  0|   0|         0|         0|         0|         0|         0|         0|         0|         0|         0|         0|       0|       0|       0|       0|       0|  0|  0|  0|  0|        0|        0|        0|        0|        0|    0|    0|\n",
      "+-----+---+---+----+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+--------+--------+--------+--------+---+---+---+---+---------+---------+---------+---------+---------+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|type|  count|\n",
      "+----+-------+\n",
      "|   3| 986539|\n",
      "|   6|2513461|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"type\").count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is no null values and the amount of galaxies are less than the amount of stars, which make sense. However, this can affect the model so we are going to balance the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, as it is a binary classification, we will update stars to 0 and galaxies to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"type\", when(col(\"type\") == 6,1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-----------+----+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+--------+--------+--------+--------+---------+---------+---------+----------+---------+---------+---------+---------+----------+---------------+-----+\n",
      "|              objID|       ra|        dec|type|petroRad_u|petroRad_g|petroRad_r|petroRad_i|petroRad_z|modelMag_u|modelMag_g|modelMag_r|modelMag_i|modelMag_z|psfMag_u|psfMag_g|psfMag_r|psfMag_i|psfMag_z|      u_g|      g_r|      r_i|       i_z|fracDeV_u|fracDeV_g|fracDeV_r|fracDeV_i| fracDeV_z|          flags|clean|\n",
      "+-------------------+---------+-----------+----+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+--------+--------+--------+--------+---------+---------+---------+----------+---------+---------+---------+---------+----------+---------------+-----+\n",
      "|1237648704596673591|228.13321|  0.1894192|   0|  2.969285|  2.969369|  2.220518|  1.249783|   1.62232|  23.71841|  23.55119|  21.70663|  21.07082|  20.94918|23.71531|24.25853|22.41145|21.53375|21.45539|0.1672249| 1.844557|0.6358051| 0.1216412|      1.0|      0.0|      1.0|      1.0|       1.0|316730484195600|    1|\n",
      "|1237648704596673595|228.13266| 0.15501632|   0|  2.969285|  1.167545|    1.7132|  4.193829|  1.581682|  26.23039|  23.06768|  21.60064|  21.04392|  20.49498|25.81252|23.30065|22.16619|21.59607|20.93345|  3.16271| 1.467041|0.5567226|  0.548933|      0.0|      1.0|0.5398329|      1.0|       0.0|    68987912448|    1|\n",
      "|1237648704596673597|228.13452| 0.18439688|   0|  4.583073|  1.801946|  1.712365|  1.611359|  1.693891|  24.13848|   22.0195|  21.33374|  21.02932|  21.22839|24.35229| 22.5369|21.95571|21.76257|21.68405| 2.118977|0.6857586|0.3044224|-0.1990643|0.2213039|      0.0|      0.0|      0.0|       0.0|    68987912448|    1|\n",
      "|1237648704596673600|228.13475|0.051330324|   0|  7.357505|  1.275892|  1.897987|  2.219621|  2.806328|  25.94563|  22.67289|  20.76974|  20.01449|   19.5253|25.25427|23.28042|21.51431|20.77017|20.30231| 3.272739| 1.903149|0.7552528|  0.489193|      1.0|      0.0|0.8176387|      1.0|0.05972257|    68987912448|    1|\n",
      "|1237648704596673601|228.13544|0.046520676|   0|  1.529451|  1.516517|  1.452753|  3.800702|  2.970232|  22.62748|  22.42105|  21.79902|  21.57086|  22.21361|22.84985|22.74921|22.23782|21.97033|22.15221|0.2064247|0.6220398|0.2281551|-0.6427536|      1.0|      1.0|      0.0|      1.0|       1.0|    68987912448|    1|\n",
      "+-------------------+---------+-----------+----+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+--------+--------+--------+--------+---------+---------+---------+----------+---------+---------+---------+---------+----------+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+---------+----------+----+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+--------+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+---------+---------+--------------+-----+\n",
      "|              objID|       ra|       dec|type|petroRad_u|petroRad_g|petroRad_r|petroRad_i|petroRad_z|modelMag_u|modelMag_g|modelMag_r|modelMag_i|modelMag_z|psfMag_u|psfMag_g|psfMag_r|psfMag_i|psfMag_z|     u_g|     g_r|      r_i|      i_z|fracDeV_u|fracDeV_g|fracDeV_r|fracDeV_i|fracDeV_z|         flags|clean|\n",
      "+-------------------+---------+----------+----+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+--------+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+---------+---------+--------------+-----+\n",
      "|1237648704596673596|228.13359|0.06267392|   1|  7.357505|  1.317162|  1.048602|  0.980907| 0.9447467|   23.2214|  21.66433|  20.29408|  19.12956|  18.53254|23.29687|21.69403|20.32223|19.14626|18.55519|1.557074|1.370243| 1.164522|0.5970211|      1.0|      1.0|      1.0|      0.0|      0.0|   69055021384|    1|\n",
      "|1237648704596673603|228.13571|0.13501696|   1| 0.7134073|  1.140595|  1.132749| 0.9277821|  0.931531|  24.37991|  22.96101|  21.76162|  20.41283|  19.70002|24.20988|22.96505|21.82184|20.45126|19.70096|1.418892|1.199396| 1.348785|0.7128162|      0.0|      1.0|      0.0|      1.0|      0.0|   68987912192|    1|\n",
      "|1237648704596673615|228.13838|0.18722917|   1|  1.083686|  1.201956| 0.9640225| 0.9422169|  1.116567|  23.75787|  22.49124|   21.1148|  20.39421|  19.97207|23.75588|22.50238|21.15819|20.42044| 20.0078|1.266636|1.376434|0.7205887| 0.422142|      1.0|      1.0|0.9983205|      0.0|      0.0|   68987912192|    1|\n",
      "|1237648704596673618|228.13933|0.21278109|   1|  2.969285|   1.09076|   1.08924| 0.9051506|  1.057727|  25.92635|  23.35114|  21.78671|  20.63976|  19.91965|25.89036|23.30975|21.77252|20.63534|19.91474|2.575216|1.564421| 1.146955|0.7201118|      0.0|      1.0|      0.0|      0.0|      0.0|   68987912448|    1|\n",
      "|1237648704596673620|228.14091|0.06415725|   1|  2.969285|  1.131206|  1.105575|  1.009262|  1.011155|  25.28518|  21.63824|  20.31469|  19.49671|  19.01631|25.20087|21.63558|20.32779|19.49734|19.02662|3.646946|1.323544|0.8179798| 0.480402|      1.0|      0.0| 0.511569|      0.0|      1.0|35253360001296|    1|\n",
      "+-------------------+---------+----------+----+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+--------+--------+--------+--------+--------+--------+---------+---------+---------+---------+---------+---------+---------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stars = df.filter(df[\"type\"] == 0)\n",
    "df_galaxies = df.filter(df[\"type\"] == 1)\n",
    "\n",
    "#Print to know the conversion is correctly done\n",
    "df_stars.show(5)\n",
    "df_galaxies.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have much more objects of one class, we need to balance the data to have a good model. Explicar m√°s el por qu√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|type| count|\n",
      "+----+------+\n",
      "|   0|986539|\n",
      "|   1|985461|\n",
      "+----+------+\n",
      "\n",
      "+-------------------+---------+-----------+----+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+--------+--------+--------+--------+---------+---------+---------+----------+---------+---------+---------+---------+----------+---------------+-----+\n",
      "|              objID|       ra|        dec|type|petroRad_u|petroRad_g|petroRad_r|petroRad_i|petroRad_z|modelMag_u|modelMag_g|modelMag_r|modelMag_i|modelMag_z|psfMag_u|psfMag_g|psfMag_r|psfMag_i|psfMag_z|      u_g|      g_r|      r_i|       i_z|fracDeV_u|fracDeV_g|fracDeV_r|fracDeV_i| fracDeV_z|          flags|clean|\n",
      "+-------------------+---------+-----------+----+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+--------+--------+--------+--------+---------+---------+---------+----------+---------+---------+---------+---------+----------+---------------+-----+\n",
      "|1237648704596673591|228.13321|  0.1894192|   0|  2.969285|  2.969369|  2.220518|  1.249783|   1.62232|  23.71841|  23.55119|  21.70663|  21.07082|  20.94918|23.71531|24.25853|22.41145|21.53375|21.45539|0.1672249| 1.844557|0.6358051| 0.1216412|      1.0|      0.0|      1.0|      1.0|       1.0|316730484195600|    1|\n",
      "|1237648704596673595|228.13266| 0.15501632|   0|  2.969285|  1.167545|    1.7132|  4.193829|  1.581682|  26.23039|  23.06768|  21.60064|  21.04392|  20.49498|25.81252|23.30065|22.16619|21.59607|20.93345|  3.16271| 1.467041|0.5567226|  0.548933|      0.0|      1.0|0.5398329|      1.0|       0.0|    68987912448|    1|\n",
      "|1237648704596673597|228.13452| 0.18439688|   0|  4.583073|  1.801946|  1.712365|  1.611359|  1.693891|  24.13848|   22.0195|  21.33374|  21.02932|  21.22839|24.35229| 22.5369|21.95571|21.76257|21.68405| 2.118977|0.6857586|0.3044224|-0.1990643|0.2213039|      0.0|      0.0|      0.0|       0.0|    68987912448|    1|\n",
      "|1237648704596673600|228.13475|0.051330324|   0|  7.357505|  1.275892|  1.897987|  2.219621|  2.806328|  25.94563|  22.67289|  20.76974|  20.01449|   19.5253|25.25427|23.28042|21.51431|20.77017|20.30231| 3.272739| 1.903149|0.7552528|  0.489193|      1.0|      0.0|0.8176387|      1.0|0.05972257|    68987912448|    1|\n",
      "|1237648704596673601|228.13544|0.046520676|   0|  1.529451|  1.516517|  1.452753|  3.800702|  2.970232|  22.62748|  22.42105|  21.79902|  21.57086|  22.21361|22.84985|22.74921|22.23782|21.97033|22.15221|0.2064247|0.6220398|0.2281551|-0.6427536|      1.0|      1.0|      0.0|      1.0|       1.0|    68987912448|    1|\n",
      "+-------------------+---------+-----------+----+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+--------+--------+--------+--------+---------+---------+---------+----------+---------+---------+---------+---------+----------+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "#Get the amount of stars and galaxies\n",
    "count_stars = df.filter(col(\"type\") == 0).count()\n",
    "count_galaxies = df.filter(col(\"type\") == 1).count()\n",
    "\n",
    "#Select the minimum number of both clases\n",
    "min_count = min(count_stars, count_galaxies)\n",
    "\n",
    "# Submuestreo: Tomar solo 'min_count' elementos de cada clase\n",
    "df_stars = df.filter(col(\"type\") == 0).sample(fraction=min_count / count_stars, seed=1)\n",
    "df_galaxies = df.filter(col(\"type\") == 1).sample(fraction=min_count / count_galaxies, seed=1)\n",
    "\n",
    "#Union the data\n",
    "df_balanced = df_stars.union(df_galaxies)\n",
    "\n",
    "#Check if everything is ok\n",
    "df_balanced.groupBy(\"type\").count().show()\n",
    "df_balanced.show(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't get exactly the same amount of data beacuse PySpark approximates the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the supervised machine model we won't use objID, ra, dec, flags and clean columns, we are going to remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: integer (nullable = false)\n",
      " |-- petroRad_u: float (nullable = true)\n",
      " |-- petroRad_g: float (nullable = true)\n",
      " |-- petroRad_r: float (nullable = true)\n",
      " |-- petroRad_i: float (nullable = true)\n",
      " |-- petroRad_z: float (nullable = true)\n",
      " |-- modelMag_u: float (nullable = true)\n",
      " |-- modelMag_g: float (nullable = true)\n",
      " |-- modelMag_r: float (nullable = true)\n",
      " |-- modelMag_i: float (nullable = true)\n",
      " |-- modelMag_z: float (nullable = true)\n",
      " |-- psfMag_u: float (nullable = true)\n",
      " |-- psfMag_g: float (nullable = true)\n",
      " |-- psfMag_r: float (nullable = true)\n",
      " |-- psfMag_i: float (nullable = true)\n",
      " |-- psfMag_z: float (nullable = true)\n",
      " |-- u_g: float (nullable = true)\n",
      " |-- g_r: float (nullable = true)\n",
      " |-- r_i: float (nullable = true)\n",
      " |-- i_z: float (nullable = true)\n",
      " |-- fracDeV_u: float (nullable = true)\n",
      " |-- fracDeV_g: float (nullable = true)\n",
      " |-- fracDeV_r: float (nullable = true)\n",
      " |-- fracDeV_i: float (nullable = true)\n",
      " |-- fracDeV_z: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ml_model = df_balanced.select(\"type\", \"petroRad_u\", \"petroRad_g\", \"petroRad_r\", \"petroRad_i\", \"petroRad_z\",\n",
    "                        \"modelMag_u\", \"modelMag_g\", \"modelMag_r\", \"modelMag_i\", \"modelMag_z\",\n",
    "                        \"psfMag_u\", \"psfMag_g\", \"psfMag_r\", \"psfMag_i\", \"psfMag_z\",\n",
    "                        \"u_g\", \"g_r\", \"r_i\", \"i_z\",\n",
    "                        \"fracDeV_u\", \"fracDeV_g\", \"fracDeV_r\", \"fracDeV_i\", \"fracDeV_z\")\n",
    "\n",
    "df_ml_model.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our objective is to create a machine learning model, we need to convert the data in a correct format: Vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "features = df.columns[1:] #We don't get the type beacuse is the result we want to get.\n",
    "assembler = VectorAssembler(inputCols = features, outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|            features|Type|\n",
      "+--------------------+----+\n",
      "|[228.133209228515...|   0|\n",
      "|[228.132659912109...|   0|\n",
      "|[228.133590698242...|   1|\n",
      "|[228.134521484375...|   0|\n",
      "|[228.134750366210...|   0|\n",
      "+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = assembler.transform(df).select(\"features\", \"Type\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to divide the dataset into train and test, so we can get the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to try different models to check which is the best for our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, LinearSVC\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(labelCol = \"Type\", featuresCol = \"features\"),\n",
    "    #\"Decision Tree\": DecisionTreeClassifier(labelCol=\"Type\", featuresCol=\"features\"),\n",
    "    #\"Random Forest\": RandomForestClassifier(labelCol=\"Type\", featuresCol=\"features\", numTrees=100),\n",
    "    #\"Gradient Boosted Trees\": GBTClassifier(labelCol=\"Type\", featuresCol=\"features\"),\n",
    "    #\"Linear SVM\": LinearSVC(labelCol=\"Type\", featuresCol=\"features\")\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"Type\", featuresCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Type\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy despu√©s de Cross Validation: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/16 18:13:04 ERROR Executor: Exception in task 7.0 in stage 5922.0 (TID 30413)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n",
      "    verify_func(obj)\n",
      "  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2201, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2174, in verify_struct\n",
      "    verifier(v)\n",
      "  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2201, in verify\n",
      "    verify_value(obj)\n",
      "  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2195, in verify_default\n",
      "    verify_acceptable_types(obj)\n",
      "  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2020, in verify_acceptable_types\n",
      "    raise PySparkTypeError(\n",
      "pyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `0` in type `int`.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/03/16 18:13:04 ERROR TaskSetManager: Task 7 in stage 5922.0 failed 1 times; aborting job\n",
      "[Stage 5922:>                                                       (0 + 2) / 8]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o34670.confusionMatrix.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 5922.0 failed 1 times, most recent failure: Lost task 7.0 in stage 5922.0 (TID 30413) (172.20.232.160 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2201, in verify\n    verify_value(obj)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2174, in verify_struct\n    verifier(v)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2201, in verify\n    verify_value(obj)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2195, in verify_default\n    verify_acceptable_types(obj)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2020, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `0` in type `int`.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass$lzycompute(MulticlassMetrics.scala:78)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass(MulticlassMetrics.scala:76)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labels$lzycompute(MulticlassMetrics.scala:241)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labels(MulticlassMetrics.scala:241)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusionMatrix(MulticlassMetrics.scala:113)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2201, in verify\n    verify_value(obj)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2174, in verify_struct\n    verifier(v)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2201, in verify\n    verify_value(obj)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2195, in verify_default\n    verify_acceptable_types(obj)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2020, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `0` in type `int`.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m metrics \u001b[38;5;241m=\u001b[39m MulticlassMetrics(predictions_rdd)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfusionMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/mllib/evaluation.py:311\u001b[0m, in \u001b[0;36mMulticlassMetrics.confusionMatrix\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.4.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconfusionMatrix\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Matrix:\n\u001b[1;32m    307\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    Returns confusion matrix: predicted classes are in columns,\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    they are ordered by class label ascending, as in \"labels\".\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfusionMatrix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/mllib/common.py:157\u001b[0m, in \u001b[0;36mJavaModelWrapper.call\u001b[0;34m(self, name, *a)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39ma: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call method of java_model\"\"\"\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallJavaFunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/mllib/common.py:131\u001b[0m, in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call Java Function\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/bigDataProject/bigdataenv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/bigDataProject/bigdataenv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o34670.confusionMatrix.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 5922.0 failed 1 times, most recent failure: Lost task 7.0 in stage 5922.0 (TID 30413) (172.20.232.160 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2201, in verify\n    verify_value(obj)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2174, in verify_struct\n    verifier(v)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2201, in verify\n    verify_value(obj)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2195, in verify_default\n    verify_acceptable_types(obj)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2020, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `0` in type `int`.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass$lzycompute(MulticlassMetrics.scala:78)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.tpByClass(MulticlassMetrics.scala:76)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labels$lzycompute(MulticlassMetrics.scala:241)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labels(MulticlassMetrics.scala:241)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusionMatrix(MulticlassMetrics.scala:113)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n    verify_func(obj)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2201, in verify\n    verify_value(obj)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2174, in verify_struct\n    verifier(v)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2201, in verify\n    verify_value(obj)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2195, in verify_default\n    verify_acceptable_types(obj)\n  File \"/home/haizeagonzalez/bigDataProject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/types.py\", line 2020, in verify_acceptable_types\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DoubleType()` can not accept object `0` in type `int`.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.01, 0.001])  # Regularizaci√≥n\n",
    "             .addGrid(lr.maxIter, [10, 20, 50])         # N√∫mero de iteraciones\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])  # ElasticNet (L1 o L2)\n",
    "             .build())\n",
    "\n",
    "# Configurar el CrossValidator\n",
    "crossval = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5  # N√∫mero de pliegues (folds) para la validaci√≥n cruzada\n",
    ")\n",
    "\n",
    "# Ajustar el modelo con CrossValidation\n",
    "cv_model = crossval.fit(train_data)\n",
    "\n",
    "# Hacer predicciones sobre el conjunto de test\n",
    "predictions = cv_model.transform(test_data)\n",
    "\n",
    "# Evaluar el modelo\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy despu√©s de Cross Validation: {accuracy}\")\n",
    "\n",
    "# Mostrar la matriz de confusi√≥n\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "predictions_rdd = predictions.select(\"prediction\", \"Type\").rdd\n",
    "metrics = MulticlassMetrics(predictions_rdd)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(metrics.confusionMatrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Accuracy = 1.0000\n",
      "Decision Tree: Accuracy = 1.0000\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    model_trained = model.fit(train_data)\n",
    "    predictions = model_trained.transform(test_data)\n",
    "    #auc = evaluator.evaluate(predictions)\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    #print(f\"{name}: AUC = {auc:.4f}\")\n",
    "    print(f\"{name}: Accuracy = {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haizeagonzalez/myproject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Confusion Matrix\n",
      "[[10563.     0.]\n",
      " [    0.  9195.]]\n",
      "Decision Tree: Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haizeagonzalez/myproject/bigdataenv/lib/python3.12/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: Confusion Matrix\n",
      "[[10563.     0.]\n",
      " [    0.  9195.]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Entrenamiento del modelo\n",
    "    model_trained = model.fit(train_data)\n",
    "\n",
    "    # Predicciones\n",
    "    predictions = model_trained.transform(test_data)\n",
    "    \n",
    "    # Evaluaci√≥n\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(f\"{name}: Accuracy = {accuracy:.4f}\")\n",
    "    \n",
    "    # Seleccionar las columnas de predicci√≥n y etiquetas\n",
    "    predictionAndLabels = predictions.select(\n",
    "        F.col(\"prediction\").cast(FloatType()), \n",
    "        F.col(\"type\").cast(FloatType())  # Asegurarse de que 'type' es float\n",
    "    )\n",
    "\n",
    "    # Convertir a RDD para usar MulticlassMetrics\n",
    "    metrics = MulticlassMetrics(predictionAndLabels.rdd.map(tuple))\n",
    "\n",
    "    # Obtener la matriz de confusi√≥n\n",
    "    conf_matrix = metrics.confusionMatrix().toArray()\n",
    "\n",
    "    # Mostrar la matriz de confusi√≥n\n",
    "    print(f\"{name}: Confusion Matrix\")\n",
    "    print(conf_matrix)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validation\n",
    "\n",
    "matriz confusion -> Si esta bien (cross validation si esta mal overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El accuracy el 100 pero puede ser que haya sido overfitting por tanto, puede ser que al haber m√°s estrellas que galaxias, prediga el que m√°s haya. Es por ello que vamos a bajar la cantidad de estrellas para que el modelo est√© balanceado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I need also de images, I have downloaded from https://skyserver.sdss.org/dr18, specifying with a request:\n",
    "- the location of the object (with right ascension (RA) and declination (dec))\n",
    "- the zoom of the picture (scale)\n",
    "- the dimmensions of the photo (with and height)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdataenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
