{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective of the project 🚀"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My objective is to create a machine learning model for object classification and then, try and compare it with a convolutional neural network (CNN) for images. For that, I will use Spark MLlib to train and evaluate the model. Secondly, create the CNN and compare both 🪐\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source of the data is: https://skyserver.sdss.org/CasJobs/\n",
    "It is necessary to register and login a user to download the data you want. Then, you have to make a query specifying:\n",
    "- Amount of rows\n",
    "- Columns\n",
    "- Where to keep the csv\n",
    "- The database\n",
    "\n",
    "As I want to get as much as possible data, I will not a maximum of rows.\n",
    "\n",
    "I also add a \"where\" so I can get only data from planets, galaxies and stars:\n",
    "- type = 3: Galaxies\n",
    "- type = 6: Stars\n",
    "\n",
    "and I downloaded the dataset to start working with it.\n",
    "\n",
    "![](notebookImages/img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns we have are:\n",
    "- objID: Unique identifier of the object → TYPE bigInt\n",
    "- ra: Right ascension → TYPE float\n",
    "- dec: Declination → TYPE float\n",
    "- petroRad: Petrosian radius, used to know the size of galaxies in astronomical pictures. It is the amount of light that a galaxy emits in a sepecific radius. Very used because it is independent of the distance and brightness. We use different photometric filters:\n",
    "    - petroRad_u: Near-ultraviolet\n",
    "    - petroRad_g: Blue-Green\n",
    "    - petroRad_r: Red\n",
    "    - petroRad_i: Near-infrared\n",
    "    - petroRad_z: Deeper infrared\n",
    " → TYPE: Real\n",
    "\n",
    "- modelMag: Brightness measure adjusted to a galaxy model. Usual for galaxies. Also for all filters (u, g, r, i and z) → TYPE Real\n",
    "- psfMag: Brightness measure based on the point source light profile. Usual for stars. Also for all filters (u, g, r, i and z) → TYPE Real\n",
    "- u_g: (modelMag_u - modelMag_g)\n",
    "- g_r: (modelMag_g - modelMag_r)\n",
    "- r_i: (modelMag_r - modelMag_i)\n",
    "- i_z: (modelMag_i - modelMag_z)\n",
    "- fracDeV: The amount of brightness that the object has in the De Vaucouleurs profile. Also for all filters (u, g, r, i and z) → TYPE Real\n",
    "- flags: Bit comination that explains different characteristics of the object. If we convert it to binary and check SDSS documentarion, we get a meaning for each bit → TYPE bigInt\n",
    "- clean: Indicator that tell us if the object was cleaned → TYPE int\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PetroRad:\n",
    "- Stars: Small and constant in all filters.\n",
    "- Galaxies: Bigger and variates depending on the wavelengths.\n",
    "\n",
    "ModelMag and psfMag:\n",
    "- In the red filter:\n",
    "    - Stars: modelMag_r ≈ psfMag_r\n",
    "    - Galaxies: modelMag_r > psfMag_r\n",
    "- In other filers:\n",
    "    - Galaxies are usuarlly more red  (modelMag_g - modelMag_r is big).\n",
    "    - Stars has different colors depending on their type.\n",
    "\n",
    "fracDeV:\n",
    "- Stars: fracDeV ≈ 0.\n",
    "- Galaxies: fracDeV ≈ 1 (eliptic) or fracDeV < 1 (espiral).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark configuration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a Spark session. I also decided to create a log in case there is any error during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"bigDataAstronomyProject\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"8\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to read de csv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "df = spark.read.csv(config.DATA_PATH, header=True)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to check if the data is correctly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema and the chacacteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all columns are strings, we need to convert them into their type. For that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"objID\", col(\"objID\").cast(\"long\")) \\\n",
    "       .withColumn(\"ra\", col(\"ra\").cast(\"float\")) \\\n",
    "       .withColumn(\"dec\", col(\"dec\").cast(\"float\")) \\\n",
    "       .withColumn(\"petroRad_u\", col(\"petroRad_u\").cast(\"float\")) \\\n",
    "       .withColumn(\"petroRad_g\", col(\"petroRad_g\").cast(\"float\")) \\\n",
    "       .withColumn(\"petroRad_r\", col(\"petroRad_r\").cast(\"float\")) \\\n",
    "       .withColumn(\"petroRad_i\", col(\"petroRad_i\").cast(\"float\")) \\\n",
    "       .withColumn(\"petroRad_z\", col(\"petroRad_z\").cast(\"float\")) \\\n",
    "       .withColumn(\"modelMag_u\", col(\"modelMag_u\").cast(\"float\")) \\\n",
    "       .withColumn(\"modelMag_g\", col(\"modelMag_g\").cast(\"float\")) \\\n",
    "       .withColumn(\"modelMag_r\", col(\"modelMag_r\").cast(\"float\")) \\\n",
    "       .withColumn(\"modelMag_i\", col(\"modelMag_i\").cast(\"float\")) \\\n",
    "       .withColumn(\"modelMag_z\", col(\"modelMag_z\").cast(\"float\")) \\\n",
    "       .withColumn(\"psfMag_u\", col(\"psfMag_u\").cast(\"float\")) \\\n",
    "       .withColumn(\"psfMag_g\", col(\"psfMag_g\").cast(\"float\")) \\\n",
    "       .withColumn(\"psfMag_r\", col(\"psfMag_r\").cast(\"float\")) \\\n",
    "       .withColumn(\"psfMag_i\", col(\"psfMag_i\").cast(\"float\")) \\\n",
    "       .withColumn(\"psfMag_z\", col(\"psfMag_z\").cast(\"float\")) \\\n",
    "       .withColumn(\"u_g\", col(\"u_g\").cast(\"float\")) \\\n",
    "       .withColumn(\"g_r\", col(\"g_r\").cast(\"float\")) \\\n",
    "       .withColumn(\"r_i\", col(\"r_i\").cast(\"float\")) \\\n",
    "       .withColumn(\"i_z\", col(\"i_z\").cast(\"float\")) \\\n",
    "       .withColumn(\"fracDeV_u\", col(\"fracDeV_u\").cast(\"float\")) \\\n",
    "       .withColumn(\"fracDeV_g\", col(\"fracDeV_g\").cast(\"float\")) \\\n",
    "       .withColumn(\"fracDeV_r\", col(\"fracDeV_r\").cast(\"float\")) \\\n",
    "       .withColumn(\"fracDeV_i\", col(\"fracDeV_i\").cast(\"float\")) \\\n",
    "       .withColumn(\"fracDeV_z\", col(\"fracDeV_z\").cast(\"float\")) \\\n",
    "       .withColumn(\"flags\", col(\"flags\").cast(\"long\")) \\\n",
    "       .withColumn(\"clean\", col(\"clean\").cast(\"int\"))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the structure, we are going to explore and clean the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, the data is cleaned because we previously selected which features we want when making the query in CasJobs. Moreover, the query uses the clear filter to get good data, with which we ensure that the data is . \n",
    "Moreover, the query uses the clear filter to get good data, with which we ensure that the data is of high quality and free from major observational errors or noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are not any null value, which is great because we do not have to change the dataset.\n",
    "\n",
    "To know the quantity of objects in each class, we are going to use a barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "number_stars = df.filter(df.type == 6).count()\n",
    "number_galaxies = df.filter(df.type == 3).count()\n",
    "\n",
    "#Convert the data into a dataframe to make the plot\n",
    "data = pd.DataFrame({\n",
    "    \"type\": [\"Stars\", \"Galaxies\"],\n",
    "    \"count\": [number_stars, number_galaxies]\n",
    "})\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(data[\"type\"], data[\"count\"], color=[\"#ff9300\", \"#42c9c9\"], edgecolor=[\"#cc7400\", \"#329999\"], linewidth=2)\n",
    "plt.xlabel(\"Object type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Total of Stars and Galaxies\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of galaxies are less than the amount of stars, which make sense."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to change \"type\" label. As it is a binary classification, we will update stars to 0 and galaxies to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"type\", when(col(\"type\") == 3,1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stars = df.filter(df[\"type\"] == 0)\n",
    "df_galaxies = df.filter(df[\"type\"] == 1)\n",
    "\n",
    "#Print to know the conversion is correctly done\n",
    "df_stars.show(5)\n",
    "df_galaxies.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to select the columns that are convenient to use them in the model. We are not going to take into account:\n",
    "- objID\n",
    "- ra\n",
    "- dec\n",
    "- flags\n",
    "- clean\n",
    "\n",
    "This is because these features are not critical to distinguish between the classes, there are just informative columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_model = df.select(\"type\", \"petroRad_u\", \"petroRad_g\", \"petroRad_r\", \"petroRad_i\", \"petroRad_z\",\n",
    "                        \"modelMag_u\", \"modelMag_g\", \"modelMag_r\", \"modelMag_i\", \"modelMag_z\",\n",
    "                        \"psfMag_u\", \"psfMag_g\", \"psfMag_r\", \"psfMag_i\", \"psfMag_z\",\n",
    "                        \"u_g\", \"g_r\", \"r_i\", \"i_z\",\n",
    "                        \"fracDeV_u\", \"fracDeV_g\", \"fracDeV_r\", \"fracDeV_i\", \"fracDeV_z\")\n",
    "\n",
    "df_ml_model.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know more about these features we are going to plot correlations matrices but making an intelligent distribution of the columns, otherwise, we will obtain a huge heatmap hard to understand and analyse. For that, we are going to get each magnitudes and each wave separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_pd = df_ml_model.toPandas()\n",
    "\n",
    "#Remove the column type, it is not numerical\n",
    "df_corr = df_pd.drop(columns=[\"type\"]).corr()\n",
    "\n",
    "groups = {\n",
    "    \"Petrosian Radio\": [\"petroRad_u\", \"petroRad_g\", \"petroRad_r\", \"petroRad_i\", \"petroRad_z\"],\n",
    "    \"Model Magnitudes\": [\"modelMag_u\", \"modelMag_g\", \"modelMag_r\", \"modelMag_i\", \"modelMag_z\"],\n",
    "    \"PSF Magnitudes\": [\"psfMag_u\", \"psfMag_g\", \"psfMag_r\", \"psfMag_i\", \"psfMag_z\"],\n",
    "    \"Color Indices\": [\"u_g\", \"g_r\", \"r_i\", \"i_z\"],\n",
    "    \"FracDeV\": [\"fracDeV_u\", \"fracDeV_g\", \"fracDeV_r\", \"fracDeV_i\", \"fracDeV_z\"],\n",
    "    \"Wave u\": [\"petroRad_u\", \"modelMag_u\", \"psfMag_u\", \"fracDeV_u\"],\n",
    "    \"Wave g\": [\"petroRad_g\", \"modelMag_g\", \"psfMag_g\", \"fracDeV_g\"],\n",
    "    \"Wave r\": [\"petroRad_r\", \"modelMag_r\", \"psfMag_r\", \"fracDeV_r\"],\n",
    "    \"Wave i\": [\"petroRad_i\", \"modelMag_i\", \"psfMag_i\", \"fracDeV_i\"],\n",
    "    \"Wave z\": [\"petroRad_z\", \"modelMag_z\", \"psfMag_z\", \"fracDeV_z\"]\n",
    "\n",
    "}\n",
    "\n",
    "def plot_heatmap(df, cols, title):\n",
    "    df_corr = df[cols].corr()  #Calculate the corralation matrix\n",
    "    mask = np.triu(np.ones_like(df_corr, dtype=bool))  #Show the half of the matrix: it is symmetrical\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(df_corr, mask=mask, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5, vmin=-1, vmax=1)\n",
    "    plt.title(f\"Heatmap of {title}\")\n",
    "    plt.show()\n",
    "\n",
    "#Plot its heatmap\n",
    "for group_name, columns in groups.items():\n",
    "    plot_heatmap(df_pd, columns, group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, if we separate each wave:\n",
    "- Variables fracDev, modelMag and psfMag are strongly positively correlated for every wave except for r wave. This means that very probable are measuring similar characteristics of the object, which is true because they measure the brightness of the object. **Galaxies** usually have high **fracDeV and modelMag** whereas **psfMag** is better to measure the **stars** brightness because they are point sources.\n",
    "- psfMag and modelMag are also fully correlated with all the waves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference behavior in the wave r it can be because of different reasons:\n",
    "- Noise in the r-band: The r-band measurements might be affected by detector noise, calibration issues, or atmospheric effects.\n",
    "- Characteristics of red light: This band is the visible light so it can happen that galaxies and stars emit similar amount of light in this band. \n",
    "\n",
    "To know which is the problem, we are going to use a boxplot to know if the band r is a good indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = ['#ff9300', '#42c9c9']\n",
    "\n",
    "ax = sns.boxplot(data=df_pd, x=\"type\", y=\"fracDeV_r\", palette=colors)\n",
    "plt.title(\"Distribution of fracDeV_r by object type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the mean of the values for type 0 (stars) is close to 1, suggesting that stars have a more compact, point-like shape. On the other hand, the values for type 1 (galaxies) are more dispersed, with a median around 0.5, indicating that galaxies are more extended and harder to distinguish using this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the heatmaps of modelMag, psfMag, and fracDeV show very similar patterns. This means that these different features are highly correlated and may be redundant when used together in a model. However, we will first train a model using all features, and then apply PCA to compare the performance of both approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set a random seed to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import set_seed\n",
    "\n",
    "\n",
    "set_seed.set_seed(132)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our objective is to create a machine learning model, we need to convert the data in a correct format: Vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "features = df_ml_model.columns[1:] #We don't get the type beacuse is the result we want to get.\n",
    "assembler = VectorAssembler(inputCols = features, outputCol = \"features\") #Convert features list as a vector\n",
    "df_ml_model = assembler.transform(df_ml_model) #Apply transformation\n",
    "df_ml_model = df_ml_model.select(\"features\", \"type\")\n",
    "\n",
    "df_ml_model.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to divide the dataset into train and test, so we can get the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_ml_model.randomSplit([0.8, 0.2], seed = 132)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to try different models to check which is the best for our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, LinearSVC\n",
    "\n",
    "#Define different models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(labelCol=\"type\", featuresCol=\"features\"),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(labelCol=\"type\", featuresCol=\"features\"),\n",
    "    \"Random Forest\": RandomForestClassifier(labelCol=\"type\", featuresCol=\"features\", numTrees=5),\n",
    "    \"Gradient Boosted Trees\": GBTClassifier(labelCol=\"type\", featuresCol=\"features\"),\n",
    "    \"Linear SVM\": LinearSVC(labelCol=\"type\", featuresCol=\"features\")\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to calculate the **AUC evaluator** for model and look how well they are doing. I decided to use AUC because it is a good technique to use for binary classification, especially when classes are unbalanced.\n",
    "\n",
    "Remember that the ROC curve gives a visual representation between the true prositive rate and false positive rate at different thresholds. So, it tell us how well the model can detect true positive and avoid false positives. Moreover, AUC is one scalar value from 0 to  that tell how the model performs globally. AUC represents the area beneath the curve.\n",
    "- AUC = 0.5 ⭢ Random Predictions\n",
    "- AUC > 0.5 ⭢ Increasingly good predictions\n",
    "- AUC = 1 ⭢ Perfect predictions\n",
    "\n",
    "\n",
    "\n",
    "![](notebookImages/img2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"type\", metricName=\"areaUnderROC\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    model_trained = model.fit(train_data)\n",
    "    predictions = model_trained.transform(test_data)\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    print(f\"{name}: AUC = {auc:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to plot the results using matplot and sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#Create the figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "#Different colors for the different models\n",
    "colors = ['b', 'g', 'r', 'c', 'm']\n",
    "\n",
    "#For each model compute the AUC\n",
    "for idx, (name, model) in enumerate(models.items()):\n",
    "    model_trained = model.fit(train_data)\n",
    "    predictions = model_trained.transform(test_data) #Predicted class (0 or 1)\n",
    "\n",
    "    #Check if the model has \"probability\" column because some of them (SVG for example) do not\n",
    "    if \"probability\" in predictions.columns:\n",
    "        #Get the whole probability vector, convert it into rdd and select the probability for the positive (real) prediction\n",
    "        prob_positives = predictions.select(\"probability\").rdd.map(lambda row: row[0][1]).collect()\n",
    "        #Get the label for each prediction\n",
    "        true_labels = predictions.select(\"type\").rdd.map(lambda row: row[0]).collect()\n",
    "\n",
    "        #Calculate ROC curve\n",
    "        fpr, tpr, _ = roc_curve(true_labels, prob_positives)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        #Plot it\n",
    "        plt.plot(fpr, tpr, color=colors[idx % len(colors)], lw=2, label=f'{name} (AUC = {roc_auc:.4f})')\n",
    "    else:\n",
    "        print(f\"The model {name} does not have 'probability' column so it will not be in the graphic.\")\n",
    "\n",
    "#Different parameters to personalize the plot\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  #Diagonal line of \"randomness\"\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC for Decision Tree is different. That is because PySpark uses BinaryClassificationEvaluator, so it is evaluating with the class prediction (star or galaxy), whereas scikit-learn uses the predicted probabilities to calculate the AUC. That is why scikit-learn should be more precise.\n",
    "\n",
    "Taking this into account, the best AUC is the one for logistic regression so we will use that model in order to make our predictions. \n",
    "\n",
    "Then, as our model has a very high AUC, we need to ensure that it is not overfitted. A high AUC does not necessarily mean good generalization; it could indicate that the model memorized the training data.\n",
    "\n",
    "To ensure about this, we will compute the confusion matrix for training and test sets. Comparing both we will know if the model is performing well or not with unseen data. We will use plots to visualize the matrices in order to make an easier and more interpetable comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"type\", featuresCol=\"features\")\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "predictions_log_train = lr_model.transform(train_data)\n",
    "predictions_log_test = lr_model.transform(test_data)\n",
    "\n",
    "auc_train = evaluator.evaluate(predictions_log_train)\n",
    "auc_test = evaluator.evaluate(predictions_log_test)\n",
    "\n",
    "print(f\"Logistic Regression (Train): AUC = {auc_train:.4f}\\n\")\n",
    "print(f\"Logistic Regression (Test): AUC = {auc_test:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the difference is almost non-existent which is good because this suggests that the model is generalizing correctly, data for train and test set are representative of the data. However, as it is a very high AUC, we can verify the confusion matrix in order to know if the model is correctly classifying. To make this, we will use the plot of the confusion matrix to visualize to get a conclusion in a easier and more interpetable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "#Divide true positives, true negatives, false positives and false negatives\n",
    "##Train\n",
    "true_galaxies_train = predictions_log_train.filter((F.col(\"type\") == 1) & (F.col(\"prediction\") == 1)).count()\n",
    "true_stars_train = predictions_log_train.filter((F.col(\"type\") == 0) & (F.col(\"prediction\") == 0)).count()\n",
    "false_stars_train = predictions_log_train.filter((F.col(\"type\") == 0) & (F.col(\"prediction\") == 1)).count()\n",
    "false_galaxies_train = predictions_log_train.filter((F.col(\"type\") == 1) & (F.col(\"prediction\") == 0)).count()\n",
    "\n",
    "#Creating the confusion matrix\n",
    "conf_matrix_values_train = [[true_stars_train, false_stars_train], [false_galaxies_train, true_galaxies_train]]\n",
    "\n",
    "\n",
    "##Test\n",
    "true_galaxies_test = predictions_log_test.filter((F.col(\"type\") == 1) & (F.col(\"prediction\") == 1)).count()\n",
    "true_stars_test = predictions_log_test.filter((F.col(\"type\") == 0) & (F.col(\"prediction\") == 0)).count()\n",
    "false_stars_test = predictions_log_test.filter((F.col(\"type\") == 0) & (F.col(\"prediction\") == 1)).count()\n",
    "false_galaxies_test = predictions_log_test.filter((F.col(\"type\") == 1) & (F.col(\"prediction\") == 0)).count()\n",
    "\n",
    "#Creating the confusion matrix\n",
    "conf_matrix_values_test = [[true_stars_test, false_stars_test], [false_galaxies_test, true_galaxies_test]]\n",
    "\n",
    "\n",
    "#Plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))  #Create two plots in 1 line\n",
    "\n",
    "#Train confusion matrix\n",
    "ax1 = sns.heatmap(np.array(conf_matrix_values_train), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                   xticklabels=[\"Stars prediction\", \"Galaxies prediction\"],\n",
    "                   yticklabels=[\"Real stars\", \"Real galaxies\"], ax=axes[0]) #Put in the axe 0, the left one\n",
    "ax1.set_xlabel(\"Prediction\")\n",
    "ax1.set_ylabel(\"Real value\")\n",
    "ax1.set_title(\"Confusion Matrix - Train\")\n",
    "\n",
    "#Test confusion matrix\n",
    "ax2 = sns.heatmap(np.array(conf_matrix_values_test), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                   xticklabels=[\"Stars prediction\", \"Galaxies prediction\"],\n",
    "                   yticklabels=[\"Real stars\", \"Real galaxies\"], ax=axes[1]) #Put in the axe 0, the right one\n",
    "ax2.set_xlabel(\"Prediction\")\n",
    "ax2.set_ylabel(\"Real value\")\n",
    "ax2.set_title(\"Confusion Matrix - Test\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect if there is or not overfitting, we need to compare the error rate in train and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train\n",
    "false_stars_train = conf_matrix_values_train[1][0]\n",
    "true_galaxies_train = conf_matrix_values_train[1][1]\n",
    "\n",
    "false_stars_train_rate = ((false_stars_train)/(false_stars_train + true_galaxies_train))*100\n",
    "false_stars_train_rate = round(false_stars_train_rate,2)\n",
    "\n",
    "\n",
    "##Test\n",
    "false_stars_test = conf_matrix_values_test[1][0]\n",
    "true_galaxies_test = conf_matrix_values_test[1][1]\n",
    "\n",
    "false_stars_test_rate = ((false_stars_test)/(false_stars_test + true_galaxies_test))*100\n",
    "false_stars_test_rate = round(false_stars_test_rate,2)\n",
    "\n",
    "\n",
    "#PLOTS\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))  #Create two plots in 1 line\n",
    "colors_train = [\"#ff9300\", \"#9b9c9c\"]\n",
    "edge_colors = [\"#cc7400\", \"#808080\"]  #Darker border colors\n",
    "\n",
    "#Train Pie Chart\n",
    "sizes_train = [100 - false_stars_train_rate, false_stars_train_rate]\n",
    "wedges_train, _, autotexts_train = axes[0].pie(\n",
    "    sizes_train, labels=[\"Correct Predictions\", \"False Stars\"], autopct='%1.1f%%', colors=colors_train, startangle=140,\n",
    "    wedgeprops={\"linewidth\": 2}  # General border thickness\n",
    ")\n",
    "\n",
    "#Set custom edge colors for Train pie chart\n",
    "for wedge, edge_color in zip(wedges_train, edge_colors):\n",
    "    wedge.set_edgecolor(edge_color)\n",
    "\n",
    "axes[0].set_title(f\"Train - False Stars Rate: {false_stars_train_rate}%\")\n",
    "\n",
    "#Test Pie Chart\n",
    "sizes_test = [100 - false_stars_test_rate, false_stars_test_rate]\n",
    "wedges_test, _, autotexts_test = axes[1].pie(\n",
    "    sizes_test, labels=[\"Correct Predictions\", \"False Stars\"], autopct='%1.1f%%', colors=colors_train, startangle=140,\n",
    "    wedgeprops={\"linewidth\": 2}  # General border thickness\n",
    ")\n",
    "\n",
    "#Set custom edge colors for Test pie chart\n",
    "for wedge, edge_color in zip(wedges_test, edge_colors):\n",
    "    wedge.set_edgecolor(edge_color)\n",
    "\n",
    "axes[1].set_title(f\"Test - False Stars Rate: {false_stars_test_rate}%\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##Train\n",
    "false_galaxies_train = conf_matrix_values_train[0][1]\n",
    "true_stars_train = conf_matrix_values_train[0][0]\n",
    "\n",
    "false_galaxies_train_rate = ((false_galaxies_train) / (false_galaxies_train + true_stars_train)) * 100\n",
    "false_galaxies_train_rate = round(false_galaxies_train_rate, 2)\n",
    "\n",
    "##Test\n",
    "false_galaxies_test = conf_matrix_values_test[0][1]\n",
    "true_stars_test = conf_matrix_values_test[0][0]\n",
    "\n",
    "false_galaxies_test_rate = ((false_galaxies_test) / (false_galaxies_test + true_stars_test)) * 100\n",
    "false_galaxies_test_rate = round(false_galaxies_test_rate, 2)\n",
    "\n",
    "\n",
    "##PLOTS\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "colors = [\"#42c9c9\", \"#9b9c9c\"] \n",
    "edge_colors = [\"#329999\", \"#808080\"]\n",
    "\n",
    "#Train \n",
    "sizes_train = [100 - false_galaxies_train_rate, false_galaxies_train_rate]\n",
    "wedges_train, _, autotexts_train = axes[0].pie(\n",
    "    sizes_train, labels=[\"Correct Predictions\", \"False Galaxies\"], autopct='%1.1f%%', colors=colors, startangle=140,\n",
    "    wedgeprops={\"linewidth\": 2} \n",
    ")\n",
    "\n",
    "#Set custom edge colors for Train pie chart\n",
    "for wedge, edge_color in zip(wedges_train, edge_colors):\n",
    "    wedge.set_edgecolor(edge_color)\n",
    "\n",
    "axes[0].set_title(f\"Train - False Galaxies Rate: {false_galaxies_train_rate}%\")\n",
    "\n",
    "#Test \n",
    "sizes_test = [100 - false_galaxies_test_rate, false_galaxies_test_rate]\n",
    "wedges_test, _, autotexts_test = axes[1].pie(\n",
    "    sizes_test, labels=[\"Correct Predictions\", \"False Galaxies\"], autopct='%1.1f%%', colors=colors, startangle=140,\n",
    "    wedgeprops={\"linewidth\": 2} \n",
    ")\n",
    "\n",
    "#Test\n",
    "for wedge, edge_color in zip(wedges_test, edge_colors):\n",
    "    wedge.set_edgecolor(edge_color)\n",
    "\n",
    "axes[1].set_title(f\"Test - False Galaxies Rate: {false_galaxies_test_rate}%\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As both rates (train and test) are very similar, the model is not overfitting so it is generalizing correctly the data. However, As the AUC value is very big, we can use **regularization** technique to make a more robust model. Regularization is intended to prevent overfitting by penalizing large coefficients and forcing the model to learn simpler, more generalizable patterns. There are different types of regularization (L1, L2 (same that L1 but squared), L1+L2, dropout,...). In this case, we will use L2 regularization because it is the most estable one and because all the selected variables are relevant. For that, it is necessary to add *regParam* to the Logistic Regression. The selected value 0.1 is because it is a simple model, it has not many variables (just 24)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_with_reg = LogisticRegression(labelCol=\"type\", featuresCol=\"features\", regParam=0.1)\n",
    "model_with_reg = lr_with_reg.fit(train_data)\n",
    "\n",
    "#Evaluate the model with regularization\n",
    "predictions_with_reg = model_with_reg.transform(test_data)\n",
    "auc_with_reg = evaluator.evaluate(predictions_with_reg)\n",
    "print(f\"Logistic Regression (Regularization): AUC = {auc_with_reg:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the AUC has decreased from 0.9919 to 0.9066 (in test). This will be good for the model to learn more generalizable patterns and not to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a technique that is used to reduce the dimensionality to simplify a large data set into a smaller one while maintaining significant patterns and trends. Trying to simplify the data we have, we are going to apply this technique. In this case, we will eliinate objID, ra, dec, flags and clean columns because we will not use it to train the models. Moreover, these features do not give us any information to classify the inspected object into a star or galaxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.drop(*[\"objID\", \"flags\", \"clean\", \"ra\", \"dec\"])\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to check the number of rows and columns in the dataset. Additionally, we will split the data into training and test sets, separating the label in each case because it is an unsupervised learning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_filtered = df_filtered.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filtered.drop(columns = [\"type\"])\n",
    "Y = df_filtered[\"type\"]\n",
    "\n",
    "n_rows, n_cols = X.shape\n",
    "\n",
    "print(f\"Number of rows: {n_rows}\")\n",
    "print(f\"Number of columns: {n_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split the data: train (80%) and test (20%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=132)\n",
    "\n",
    "print(\"Size of X_train:\", X_train.shape)\n",
    "print(\"Size of X_test:\", X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to normalize/standardize the data so all variables are in the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) #We fit just with the train data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to calculate the accumulated explained variance and the variance of each component of PCA to select the k optimal value and reduce the dimensions of the data to apply in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=n_cols)  #Taking into account that the number of components will be the matrix rank\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "exp_variance = pd.DataFrame(data=pca.explained_variance_ratio_,\n",
    "                            index=['PC' + str(n_pca + 1) for n_pca in range(pca.n_components)],\n",
    "                            columns=['Exp_variance'])\n",
    "\n",
    "exp_variance['cum_Exp_variance'] = exp_variance['Exp_variance'].cumsum()\n",
    "exp_variance.head(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the threshold if we want to have more or less variance in the model. Taking this value into account, the plot will tell us the optimal value to reach that variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Threshold to know the acumulated variances\n",
    "threshold = 0.95\n",
    "\n",
    "#Get the first index fir k where we get the selected theshold\n",
    "optimal_k = exp_variance[exp_variance['cum_Exp_variance'] >= threshold].index[0]\n",
    "\n",
    "#Plot it\n",
    "sns.barplot(data=exp_variance, x=exp_variance.index, y='Exp_variance', color='gray')\n",
    "sns.lineplot(data=exp_variance, x=exp_variance.index, y='cum_Exp_variance', color='blue', label='Cumulative Proportion')\n",
    "plt.axvline(x=optimal_k, color='red', linestyle='--', label=f'Optimal k = {optimal_k}')\n",
    "\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal k (number of components) based on {threshold*100}% variance: {optimal_k}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also try using the log-likelihood to find the best k value. To do this, we will define a function to calculate the log-likelihood based on the explained variances obtained from the PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def log_likelihood(evals):\n",
    "\n",
    "    Lmax = len(evals)\n",
    "    ll = np.arange(0.0, Lmax)\n",
    "\n",
    "    for L in range(Lmax):\n",
    "\n",
    "        group1 = evals[0 : L + 1]  # Divide Eigenvalues in two groups\n",
    "        group2 = evals[L + 1 : Lmax]\n",
    "\n",
    "        mu1 = np.mean(group1)\n",
    "        mu2 = np.mean(group2)\n",
    "\n",
    "        # eqn (20.30)\n",
    "        sigma = (np.sum((group1 - mu1) ** 2) + np.sum((group2 - mu2) ** 2)) / Lmax\n",
    "\n",
    "        ll_group1 = np.sum(multivariate_normal.logpdf(group1, mu1, sigma))\n",
    "        ll_group2 = np.sum(multivariate_normal.logpdf(group2, mu2, sigma))\n",
    "\n",
    "        ll[L] = ll_group1 + ll_group2  \n",
    "    return ll"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = pca.explained_variance_\n",
    "\n",
    "ll = log_likelihood(evals)\n",
    "\n",
    "max_k = min(50, len(evals)) #We are taking as much 50 components. If there are less, we get that number\n",
    "xs = np.arange(1, max_k + 1) #Array that contains the labels for x axis: [0, 1, 2,...]\n",
    "ys = ll[:max_k] #Gets the log-likelihood for the max_k evaluated components\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(xs, ys, marker='o', label='Log-likelihood') #Curve\n",
    "idx = np.argmax(ys) #Maximum value in ys (biggest log-likelihood)\n",
    "plt.axvline(xs[idx], color='red', linestyle='--', label=f'Best k = {xs[idx]}') #Vertical line for the best k\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Profile Log-Likelihood\")\n",
    "plt.title(\"Optimal number of PCs based on log-likelihood\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal k (based on log-likelihood): {int(xs[idx])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparativa\n",
    "print(f\"Optimal k (Log-likelihood): {xs[idx]}\")\n",
    "print(f\"Optimal k (number of components) based on {threshold*100}% variance: {optimal_k}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the optimal k is different for each technique. This is because the first component (PC1) explains much more variance comparing to the rest, and the log-likelihood model assumes that the other values are noise. However, as the first component just explains the 42% of the variance, we will keep the k=10."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to know which features are those that are more important for each component taking into account the k number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying this into a model, we will analyse some chacteristics of the PCA. First, we are going to check which are the more important features for each component. For that, we are going to calculate the different loadings for each component and select the top 5 features (in absolute value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "k_number = int(optimal_k.replace('PC', '')) \n",
    "\n",
    "pca = PCA(n_components=k_number)\n",
    "pca.fit(X)  #Adjust PCA to our data\n",
    "\n",
    "#Get the loads of the principal components\n",
    "component_loadings = pd.DataFrame(pca.components_.T, \n",
    "                                  columns=[f'PC{i+1}' for i in range(pca.n_components_)], \n",
    "                                  index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "top_features = {}\n",
    "\n",
    "n_components = component_loadings.shape[1]\n",
    "\n",
    "#Get the top 5 characteristics for each component, those more important (give more information to de component)\n",
    "for i in range(n_components):\n",
    "    top_features[f'PC{i+1}'] = component_loadings.abs().nlargest(5, f'PC{i+1}').index\n",
    "\n",
    "top_features_df = pd.DataFrame(top_features)\n",
    "top_features_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to plot the top 5 characteristics for PC1 and PC2 to know the weight of each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Get top 5 features for each component (absolute values)\n",
    "top_pc1 = component_loadings['PC1'].abs().nlargest(5).index\n",
    "top_pc2 = component_loadings['PC2'].abs().nlargest(5).index\n",
    "\n",
    "#Create the DataFrame\n",
    "plot_data = []\n",
    "for feature in set(top_pc1).union(set(top_pc2)):\n",
    "    row = {'Feature': feature}\n",
    "    if feature in top_pc1:\n",
    "        row.update({'PC1_Value': component_loadings.loc[feature, 'PC1']})\n",
    "    if feature in top_pc2:\n",
    "        row.update({'PC2_Value': component_loadings.loc[feature, 'PC2']})\n",
    "    plot_data.append(row)\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data).set_index('Feature')\n",
    "\n",
    "COLOR_SCHEME = {\n",
    "    'PC1+': '#dc85ff',\n",
    "    'PC1-': '#6d0d80', \n",
    "    'PC2+': '#c2c2c2',\n",
    "    'PC2-': '#8a8a8a'\n",
    "}\n",
    "\n",
    "#Change NaNs to 0\n",
    "features = plot_df.index\n",
    "pc1_values = plot_df['PC1_Value'].fillna(0)\n",
    "pc2_values = plot_df['PC2_Value'].fillna(0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "bar_width = 0.35\n",
    "y_pos = np.arange(len(features))\n",
    "\n",
    "#Bars for PC1\n",
    "pc1_bars = ax.barh(y=y_pos - bar_width/2, width=pc1_values, \n",
    "                   height=bar_width, label='PC1')\n",
    "#Bars for PC2\n",
    "pc2_bars = ax.barh(y=y_pos + bar_width/2, width=pc2_values, \n",
    "                   height=bar_width, label='PC2')\n",
    "\n",
    "#Assign colors\n",
    "for bar, value in zip(pc1_bars, pc1_values):\n",
    "    color = COLOR_SCHEME['PC1+'] if value > 0 else COLOR_SCHEME['PC1-']\n",
    "    bar.set_color(color)\n",
    "\n",
    "for bar, value in zip(pc2_bars, pc2_values):\n",
    "    color = COLOR_SCHEME['PC2+'] if value > 0 else COLOR_SCHEME['PC2-']\n",
    "    bar.set_color(color)\n",
    "\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(features)\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_title('Top 5 Features for PC1 and PC2 (Absolute Loading)')\n",
    "ax.set_xlabel('Loading Value')\n",
    "ax.axvline(0, color='black', linewidth=0.5)\n",
    "\n",
    "#Legend\n",
    "legend_elements = [\n",
    "    plt.Rectangle((0,0), 1, 1, color=COLOR_SCHEME['PC1+'], label='PC1 Positive'),\n",
    "    plt.Rectangle((0,0), 1, 1, color=COLOR_SCHEME['PC1-'], label='PC1 Negative'),\n",
    "    plt.Rectangle((0,0), 1, 1, color=COLOR_SCHEME['PC2+'], label='PC2 Positive'),\n",
    "    plt.Rectangle((0,0), 1, 1, color=COLOR_SCHEME['PC2-'], label='PC2 Negative')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right', \n",
    "          title='Component Directions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this plot we can see which are the top 5 features for each component. For example: fracDev_i is approximately +0.4 for PC2, which means that this feature contributes positively to that component; contributes negatively those with negative values. With this plot we can understand which characteristics are most strongly correlated with the first two components (PC1 and PC2). These are the features that influence the data the most in the context of PCA. Features with high positive or negative values in PC1 and PC2 will have a significant impact on the way the data is represented in the new, reduced-dimensional space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we obtain the optimal value of k, we will create a Spark DataFrame to use it in Spark ML. We need to remove the type column, as we will be applying PCA, which is an unsupervised learning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, PCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "#We have to create a DataFrame from Spark to use it in the models\n",
    "df_spark = spark.createDataFrame(df_filtered)\n",
    "\n",
    "features_pca = df_spark.columns[1:]#We don't get the type beacuse is the result we want to get.\n",
    "\n",
    "assembler_pca = VectorAssembler(inputCols = features_pca, outputCol = \"features\") #Convert features list as a vector\n",
    "df_ml_model_pca = assembler_pca.transform(df_spark) #Apply transformation\n",
    "df_ml_model_pca = df_ml_model_pca.select(\"features\", \"type\")\n",
    "\n",
    "df_ml_model_pca.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to follow the exactly same steps than before (without PCA) but when training the models, we are going to use a pipeline to determine that before training it, we are going to apply PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pca, test_data_pca = df_ml_model_pca.randomSplit([0.8, 0.2], seed = 132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define different models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(labelCol=\"type\", featuresCol=\"features\"),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(labelCol=\"type\", featuresCol=\"features\"),\n",
    "    \"Random Forest\": RandomForestClassifier(labelCol=\"type\", featuresCol=\"features\", numTrees=5),\n",
    "    \"Gradient Boosted Trees\": GBTClassifier(labelCol=\"type\", featuresCol=\"features\"),\n",
    "    \"Linear SVM\": LinearSVC(labelCol=\"type\", featuresCol=\"features\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define PCA with the optimized k value\n",
    "pca = PCA(k=k_number, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "\n",
    "#Select the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"type\", metricName=\"areaUnderROC\")\n",
    "\n",
    "#Apply PCA and train the models\n",
    "for name, model in models.items():\n",
    "    #Create the pipeline so first it will apply PCA and the the model\n",
    "    pipeline = Pipeline(stages=[pca, model])\n",
    "    \n",
    "    #Fit the model with the train data\n",
    "    model_trained = pipeline.fit(train_data_pca)\n",
    "    \n",
    "    #Make predictions with the test data\n",
    "    predictions = model_trained.transform(test_data_pca)\n",
    "    \n",
    "    #Evaluate the model the same way than before\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    print(f\"{name}: AUC = {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#Create the figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "#Different colors for the different models\n",
    "colors = ['b', 'g', 'r', 'c', 'm']\n",
    "\n",
    "#For each model compute the AUC\n",
    "for idx, (name, model) in enumerate(models.items()):\n",
    "    model_trained = model.fit(train_data_pca)\n",
    "    predictions = model_trained.transform(test_data_pca) #Predicted class (0 or 1)\n",
    "\n",
    "    #Check if the model has \"probability\" column because some of them (SVG for example) do not\n",
    "    if \"probability\" in predictions.columns:\n",
    "        #Get the whole probability vector, convert it into rdd and select the probability for the positive (real) prediction\n",
    "        prob_positives = predictions.select(\"probability\").rdd.map(lambda row: row[0][1]).collect()\n",
    "        #Get the label for each prediction\n",
    "        true_labels = predictions.select(\"type\").rdd.map(lambda row: row[0]).collect()\n",
    "\n",
    "        #Calculate ROC curve\n",
    "        fpr, tpr, _ = roc_curve(true_labels, prob_positives)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        #Plot it\n",
    "        plt.plot(fpr, tpr, color=colors[idx % len(colors)], lw=2, label=f'{name} (AUC = {roc_auc:.4f})')\n",
    "    else:\n",
    "        print(f\"The model {name} does not have 'probability' column so it will not be in the graphic.\")\n",
    "\n",
    "#Different parameters to personalize the plot\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  #Diagonal line of \"random\"\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtained results \n",
    "Results obtained with **k = 10 → 95%** variance explained\n",
    "\n",
    "**Logistic Regression**\n",
    "- Without PCA: AUC = 0.9920\n",
    "- PCA: AUC = 0.9918\n",
    "\n",
    "**Decision Tree**\n",
    "- Without PCA: AUC = 0.8188 \n",
    "- PCA: AUC = 0.8120\n",
    "\n",
    "**Random Forest**\n",
    "- Without PCA: AUC = 0.9737\n",
    "- PCA: AUC = 0.9737\n",
    "\n",
    "**Gradient Boosted Trees**\n",
    "- Without PCA: AUC = 0.9870 \n",
    "- PCA: AUC = 0.9872\n",
    "\n",
    "**Linear SVM**\n",
    "- Without PCA: AUC = 0.9887 \n",
    "- PCA: AUC = 0.9910\n",
    "\n",
    "As we can see, all the results are very similar. However with PCA we obtained a better AUC value in:\n",
    "- Decision Tree\n",
    "- Linear SVM\n",
    "\n",
    "The reason of this can be because both methods are sensible to the amount and quality of features and when applying PCA, we are removing noisy or rdeundant characteristics so we are improving the performance of the models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the best model we get is the logistic regression. We are going to follow the same procedure to ensure that the model is not overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"type\", featuresCol=\"features\")\n",
    "lr_model = lr.fit(train_data_pca)\n",
    "\n",
    "predictions_log_train_pca = lr_model.transform(train_data_pca)\n",
    "predictions_log_test_pca = lr_model.transform(test_data_pca)\n",
    "\n",
    "auc_train_pca = evaluator.evaluate(predictions_log_train_pca)\n",
    "auc_test_pca = evaluator.evaluate(predictions_log_test_pca)\n",
    "\n",
    "print(f\"Logistic Regression (Train): AUC = {auc_train_pca:.4f}\\n\")\n",
    "print(f\"Logistic Regression (Test): AUC = {auc_test_pca:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, it is a very high AUC so we have to check that the model is not overfitted. For that we will look the confusion matrix and the error percentage for training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "#Divide true positives, true negatives, false positives and false negatives\n",
    "##Train\n",
    "true_galaxies_train_pca = predictions_log_train_pca.filter((F.col(\"type\") == 1) & (F.col(\"prediction\") == 1)).count()\n",
    "true_stars_train_pca = predictions_log_train_pca.filter((F.col(\"type\") == 0) & (F.col(\"prediction\") == 0)).count()\n",
    "false_stars_train_pca = predictions_log_train_pca.filter((F.col(\"type\") == 0) & (F.col(\"prediction\") == 1)).count()\n",
    "false_galaxies_train_pca = predictions_log_train_pca.filter((F.col(\"type\") == 1) & (F.col(\"prediction\") == 0)).count()\n",
    "\n",
    "#Creating the confusion matrix\n",
    "conf_matrix_values_train_pca = [[true_stars_train_pca, false_stars_train_pca], [false_galaxies_train_pca, true_galaxies_train_pca]]\n",
    "\n",
    "\n",
    "##Test\n",
    "true_galaxies_test_pca = predictions_log_test_pca.filter((F.col(\"type\") == 1) & (F.col(\"prediction\") == 1)).count()\n",
    "true_stars_test_pca = predictions_log_test_pca.filter((F.col(\"type\") == 0) & (F.col(\"prediction\") == 0)).count()\n",
    "false_stars_test_pca = predictions_log_test_pca.filter((F.col(\"type\") == 0) & (F.col(\"prediction\") == 1)).count()\n",
    "false_galaxies_test_pca = predictions_log_test_pca.filter((F.col(\"type\") == 1) & (F.col(\"prediction\") == 0)).count()\n",
    "\n",
    "#Creating the confusion matrix\n",
    "conf_matrix_values_test_pca = [[true_stars_test_pca, false_stars_test_pca], [false_galaxies_test_pca, true_galaxies_test_pca]]\n",
    "\n",
    "\n",
    "##PLOTS\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))  #Create two plots in 1 line\n",
    "\n",
    "#Train confusion matrix\n",
    "ax1 = sns.heatmap(np.array(conf_matrix_values_train_pca), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                   xticklabels=[\"Stars prediction\", \"Galaxies prediction\"],\n",
    "                   yticklabels=[\"Real stars\", \"Real galaxies\"], ax=axes[0]) #Put in the axe 0, the left one\n",
    "ax1.set_xlabel(\"Prediction\")\n",
    "ax1.set_ylabel(\"Real value\")\n",
    "ax1.set_title(\"Confusion Matrix - Train\")\n",
    "\n",
    "#Test confusion matrix\n",
    "ax2 = sns.heatmap(np.array(conf_matrix_values_test_pca), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                   xticklabels=[\"Stars prediction\", \"Galaxies prediction\"],\n",
    "                   yticklabels=[\"Real stars\", \"Real galaxies\"], ax=axes[1]) #Put in the axe 0, the right one\n",
    "ax2.set_xlabel(\"Prediction\")\n",
    "ax2.set_ylabel(\"Real value\")\n",
    "ax2.set_title(\"Confusion Matrix - Test\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Train\n",
    "false_stars_train_pca = conf_matrix_values_train_pca[1][0]\n",
    "true_galaxies_train_pca = conf_matrix_values_train_pca[1][1]\n",
    "\n",
    "false_stars_train_rate_pca = ((false_stars_train_pca) / (false_stars_train_pca + true_galaxies_train_pca)) * 100\n",
    "false_stars_train_rate_pca = round(false_stars_train_rate_pca, 2)\n",
    "\n",
    "#Test\n",
    "false_stars_test_pca = conf_matrix_values_test_pca[1][0]\n",
    "true_galaxies_test_pca = conf_matrix_values_test_pca[1][1]\n",
    "\n",
    "false_stars_test_rate_pca = ((false_stars_test_pca) / (false_stars_test_pca + true_galaxies_test_pca)) * 100\n",
    "false_stars_test_rate_pca = round(false_stars_test_rate_pca, 2)\n",
    "\n",
    "\n",
    "##PLOTS\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "colors_train = [\"#ff9300\", \"#9b9c9c\"]\n",
    "edge_colors = [\"#cc7400\", \"#808080\"]\n",
    "\n",
    "#Train\n",
    "sizes_train = [100 - false_stars_train_rate_pca, false_stars_train_rate_pca]\n",
    "wedges_train, _, autotexts_train = axes[0].pie(\n",
    "    sizes_train, labels=[\"Correct Predictions\", \"False Stars\"], autopct='%1.1f%%', colors=colors_train, startangle=140,\n",
    "    wedgeprops={\"linewidth\": 2}  # General border thickness\n",
    ")\n",
    "\n",
    "for wedge, edge_color in zip(wedges_train, edge_colors):\n",
    "    wedge.set_edgecolor(edge_color)\n",
    "\n",
    "axes[0].set_title(f\"Train - False Stars Rate: {false_stars_train_rate_pca}%\")\n",
    "\n",
    "\n",
    "#Test\n",
    "sizes_test = [100 - false_stars_test_rate_pca, false_stars_test_rate_pca]\n",
    "wedges_test, _, autotexts_test = axes[1].pie(\n",
    "    sizes_test, labels=[\"Correct Predictions\", \"False Stars\"], autopct='%1.1f%%', colors=colors_train, startangle=140,\n",
    "    wedgeprops={\"linewidth\": 2}  # General border thickness\n",
    ")\n",
    "\n",
    "for wedge, edge_color in zip(wedges_test, edge_colors):\n",
    "    wedge.set_edgecolor(edge_color)\n",
    "\n",
    "axes[1].set_title(f\"Test - False Stars Rate: {false_stars_test_rate_pca}%\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Train\n",
    "false_galaxies_train_pca = conf_matrix_values_train_pca[0][1]\n",
    "true_stars_train_pca = conf_matrix_values_train_pca[0][0]\n",
    "\n",
    "false_galaxies_train_rate_pca = ((false_galaxies_train_pca) / (false_galaxies_train_pca + true_stars_train_pca)) * 100\n",
    "false_galaxies_train_rate_pca = round(false_galaxies_train_rate_pca, 2)\n",
    "\n",
    "#Test\n",
    "false_galaxies_test_pca = conf_matrix_values_test_pca[0][1]\n",
    "true_stars_test_pca = conf_matrix_values_test_pca[0][0]\n",
    "\n",
    "false_galaxies_test_rate_pca = ((false_galaxies_test_pca) / (false_galaxies_test_pca + true_stars_test_pca)) * 100\n",
    "false_galaxies_test_rate_pca = round(false_galaxies_test_rate_pca, 2)\n",
    "\n",
    "\n",
    "##PLOTS\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "colors = [\"#42c9c9\", \"#9b9c9c\"] \n",
    "edge_colors = [\"#329999\", \"#808080\"] \n",
    "\n",
    "#Train\n",
    "sizes_train = [100 - false_galaxies_train_rate_pca, false_galaxies_train_rate_pca]\n",
    "wedges_train, _, autotexts_train = axes[0].pie(\n",
    "    sizes_train, labels=[\"Correct Predictions\", \"False Galaxies\"], autopct='%1.1f%%', colors=colors, startangle=140,\n",
    "    wedgeprops={\"linewidth\": 2}  # General border thickness\n",
    ")\n",
    "\n",
    "for wedge, edge_color in zip(wedges_train, edge_colors):\n",
    "    wedge.set_edgecolor(edge_color)\n",
    "\n",
    "axes[0].set_title(f\"Train - False Galaxies Rate: {false_galaxies_train_rate_pca}%\")\n",
    "\n",
    "\n",
    "#Test\n",
    "sizes_test = [100 - false_galaxies_test_rate_pca, false_galaxies_test_rate_pca]\n",
    "wedges_test, _, autotexts_test = axes[1].pie(\n",
    "    sizes_test, labels=[\"Correct Predictions\", \"False Galaxies\"], autopct='%1.1f%%', colors=colors, startangle=140,\n",
    "    wedgeprops={\"linewidth\": 2}\n",
    ")\n",
    "\n",
    "for wedge, edge_color in zip(wedges_test, edge_colors):\n",
    "    wedge.set_edgecolor(edge_color)\n",
    "\n",
    "axes[1].set_title(f\"Test - False Galaxies Rate: {false_galaxies_test_rate_pca}%\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the errors are similar in training and testing, which means that the model is not overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I need also de images, I have downloaded from https://skyserver.sdss.org/dr18, specifying with a request:\n",
    "- the location of the object (with right ascension (RA) and declination (dec))\n",
    "- the zoom of the picture (scale)\n",
    "- the dimmensions of the photo (with and height)\n",
    "\n",
    "These images will be saved into a \"images\" folder splitted into train and test and each one splitted as galaxies or stars. In a visual way would be:\n",
    "\n",
    "![](notebookImages/folderSchema.png)\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python download_images.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First it is necessary to know the amount of channels we are going to work with. For that, we are going to take one photo and verify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import config\n",
    "\n",
    "image_path = config.IMAGES_PATH + \"/train/star\"\n",
    "\n",
    "#Get all the images\n",
    "images = [f for f in os.listdir(image_path) if f.endswith((\".jpg\"))]\n",
    "\n",
    "image_path = os.path.join(image_path, images[0]) #Get the first photo\n",
    "\n",
    "image = cv2.imread(image_path) #Read the first photo\n",
    "\n",
    "if len(image.shape) == 2:\n",
    "    print(\"Image with grayscale.\")\n",
    "elif len(image.shape) == 3 and image.shape[2] == 3:\n",
    "    print(\"Image with 3 channels (RGB).\")\n",
    "elif len(image.shape) == 3 and image.shape[2] == 4:\n",
    "    print(\"Image with 4 channels (RGBA).\") #RGB + Alpha (opacity of the image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our images have 3 channels RGB. Now, we are going to normalize the pixels values  because we are going to work with a neural network and usually this improves the model performance and facilitate the process of learning. For that we need to calculate the mean and std for our images, and then apply those values to normalize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def calculate_mean_std(dataloader):\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    total_images = 0\n",
    "\n",
    "    for img, _ in dataloader:\n",
    "        batch_samples = img.size(0)  #Amount of images per batch\n",
    "        img = img.view(batch_samples, img.size(1), -1)  #Reorganize the tensor of images. If img has the shape\n",
    "        #of (batch, channels, height, width), it will be flatted; which means that all pixels of a channel will\n",
    "        #be put in a single dimension so we can obtain the mean and the std.\n",
    "        mean += img.mean(2).mean(0)  #Mean by channel\n",
    "        std += img.std(2).std(0)  #std per channel\n",
    "\n",
    "        total_images += batch_samples\n",
    "\n",
    "    mean /= total_images\n",
    "    std /= total_images\n",
    "    return mean, std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to transform the images: Convert them into tensors and apply normalization. After that, we are going to get the train, test and validation datasets and also, use them to create the DataLoaders. The used batch size was to start with something, but then we will try to optimize the hyperparameters using a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Dataset and loader for all the images\n",
    "full_dataset = datasets.ImageFolder(config.IMAGES_PATH, transform=transforms.ToTensor())\n",
    "full_loader = DataLoader(full_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "#Calculate the mean and std to normalize the images\n",
    "mean, std = calculate_mean_std(full_loader)\n",
    "\n",
    "#Transform the images (convert them into tensors + normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean.tolist(), std=std.tolist())\n",
    "])\n",
    "\n",
    "#Get the train, test and validation datasets\n",
    "train_dataset = datasets.ImageFolder(root=config.IMAGES_PATH+\"/train\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=config.IMAGES_PATH+\"/test\", transform=transform)\n",
    "validation_dataset = datasets.ImageFolder(root=config.IMAGES_PATH+\"/validation\", transform=transform)\n",
    "\n",
    "#Create the DataLoader for train, test and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will verify if the images are correctly imported and transformed (convert them into tensors and normalize them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def show_images(data_loader, mean, std, num_images):\n",
    "    data_iter = iter(data_loader) #Convert dataLoader into an iterable\n",
    "    images, labels = next(data_iter) #Get the next batch (a set of images processed simultaneously)\n",
    "\n",
    "    #Plot to show images\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 15))\n",
    "    \n",
    "    #Ensure that the mean and std is arrays of numpy\n",
    "    mean = np.array(mean.tolist())\n",
    "    std = np.array(std.tolist())\n",
    "    \n",
    "    #For each image\n",
    "    for i in range(num_images):\n",
    "        image = images[i].permute(1, 2, 0).numpy()  #Change the format of images, matplolib needs (H, W, C)\n",
    "        \n",
    "        #Undo the normalization to see correctly the images\n",
    "        image = image * std + mean\n",
    "        \n",
    "        image = np.clip(image, 0, 1) #Ensure that images pixels are between 0 and 1 values (to see correctly the image)\n",
    "        #If a value is less than 0, that pixel would be fully black or with errors.\n",
    "        #If a value is bigger than 1 the colors could be saturated.\n",
    "        \n",
    "        #Plot the image\n",
    "        ax = axes[i]\n",
    "        ax.imshow(image)\n",
    "        ax.set_title(f\"Label: {labels[i].item()}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "show_images(train_loader, mean, std, num_images=8)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to see the amount of data we have for train, test and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "class_names = full_dataset.classes  #Class names list\n",
    "class_counts = {class_name: 0 for class_name in class_names}  #Create the dictionary to count the amount of images for each class\n",
    "\n",
    "#Count images by class\n",
    "for _, label in full_dataset.samples:  #List (image_path, label)\n",
    "    class_name = class_names[label]\n",
    "    class_counts[class_name] += 1\n",
    "\n",
    "#Convert it into a dataFrame\n",
    "data = pd.DataFrame({\n",
    "    \"Type\": list(class_counts.keys()),\n",
    "    \"Count\": list(class_counts.values())\n",
    "})\n",
    "\n",
    "#Calculate the percentage\n",
    "total_count = data[\"Count\"].sum()\n",
    "data[\"Percentage\"] = (data[\"Count\"] / total_count) * 100\n",
    "\n",
    "colors = [\"#d7bde2\", \"#abebc6\", \"#f2d7d5\"] \n",
    "edge_colors = [\"#a97bb0\", \"#7fbf94\", \"#D9A7A3\"]\n",
    "\n",
    "##PLOT\n",
    "plt.figure(figsize=(8, 8))\n",
    "wedges, _, autotexts = plt.pie(\n",
    "    data[\"Count\"],\n",
    "    labels=data[\"Type\"],\n",
    "    autopct=\"%1.1f%%\",\n",
    "    colors=colors,\n",
    "    startangle=140,\n",
    "    wedgeprops={\"linewidth\": 2}\n",
    ")\n",
    "\n",
    "#Set the borders colors\n",
    "for wedge, edge_color in zip(wedges, edge_colors):\n",
    "    wedge.set_edgecolor(edge_color)\n",
    "\n",
    "plt.title(\"Distribution of data by sets\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we did for the CSV, we will count the amount of stars and galaxies we have. As the data is splitted into train, test and validation, we have to count them separately and the join them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_names = train_dataset.classes  #List with class names\n",
    "class_counts = {class_name: 0 for class_name in class_names}  #Create the dictionary to count the amount of images for each class\n",
    "\n",
    "#Function to count images and generate the DataFrame\n",
    "def get_class_counts(dataset):\n",
    "    class_names = dataset.classes  #List with class names (galaxy, star)\n",
    "    class_counts = {class_name: 0 for class_name in class_names}  #Create dictionary to count images per class\n",
    "\n",
    "    #Count images by class\n",
    "    for _, label in dataset.samples:  #List (image_path, label)\n",
    "        class_name = class_names[label]\n",
    "        class_counts[class_name] += 1\n",
    "\n",
    "    #Convert to DataFrame and calculate percentage\n",
    "    data = pd.DataFrame({\n",
    "        \"Type\": list(class_counts.keys()),\n",
    "        \"Count\": list(class_counts.values())\n",
    "    })\n",
    "\n",
    "    return data\n",
    "\n",
    "#Get the DataFrames for each dataset\n",
    "train_data = get_class_counts(train_dataset)\n",
    "val_data = get_class_counts(validation_dataset)\n",
    "test_data = get_class_counts(test_dataset)\n",
    "\n",
    "\n",
    "# Merge the data from the three datasets into a single DataFrame\n",
    "combined_data = pd.merge(train_data, val_data, on=\"Type\", suffixes=('_Train', '_Validation'))\n",
    "combined_data = pd.merge(combined_data, test_data.rename(columns={\"Count\": \"Count_Test\"}), on=\"Type\")\n",
    "\n",
    "# Now you can calculate the totals\n",
    "stars_total = combined_data[combined_data[\"Type\"] == \"star\"]['Count_Train'].sum() + combined_data[combined_data[\"Type\"] == \"star\"]['Count_Validation'].sum() + combined_data[combined_data[\"Type\"] == \"star\"]['Count_Test'].sum()\n",
    "galaxies_total = combined_data[combined_data[\"Type\"] == \"galaxy\"]['Count_Train'].sum() + combined_data[combined_data[\"Type\"] == \"galaxy\"]['Count_Validation'].sum() + combined_data[combined_data[\"Type\"] == \"galaxy\"]['Count_Test'].sum()\n",
    "\n",
    "# Create the DataFrame with the total counts\n",
    "total_data = pd.DataFrame({\n",
    "    \"Type\": [\"Stars\", \"Galaxies\"],\n",
    "    \"Total Count\": [stars_total, galaxies_total]\n",
    "})\n",
    "\n",
    "# Reverse the DataFrame to match the previous shape\n",
    "\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(total_data[\"Type\"], total_data[\"Total Count\"], color=[\"#ff9300\", \"#42c9c9\"], edgecolor=[\"#cc7400\", \"#329999\"], linewidth=2)\n",
    "plt.xlabel(\"Object Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Total of Stars and Galaxies (Images)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know if the distribution of images is the same for all the sets, we are going to use a pie chart and the percentages of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Function to count images and generate the DataFrame\n",
    "def add_percentage(data):\n",
    "    data[\"Percentage\"] = (data[\"Count\"] / data[\"Count\"].sum()) * 100\n",
    "    return data\n",
    "\n",
    "train_data = add_percentage(train_data)\n",
    "val_data = add_percentage(val_data)\n",
    "test_data = add_percentage(test_data)\n",
    "\n",
    "#Datasets and titles\n",
    "datasets = [(train_data, \"Training Set\"), (val_data, \"Validation Set\"), (test_data, \"Test Set\")]\n",
    "\n",
    "\n",
    "##PLOT\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "colors = [\"#42c9c9\", \"#ff9300\"]\n",
    "edge_colors = [\"#329999\", \"#cc7400\"]\n",
    "\n",
    "\n",
    "#Plot each dataset in a subplot\n",
    "for ax, (data, title) in zip(axes, datasets):\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        data[\"Count\"],\n",
    "        labels=data[\"Type\"],\n",
    "        autopct=\"%1.1f%%\",\n",
    "        colors=colors,\n",
    "        startangle=140,\n",
    "        wedgeprops={\"linewidth\": 2}\n",
    "    )\n",
    "\n",
    "    #Set the color for the border\n",
    "    for wedge, edge_color in zip(wedges, edge_colors):\n",
    "        wedge.set_edgecolor(edge_color)\n",
    "\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.suptitle(\"Distribution of Stars and Galaxies in Datasets\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to try 2 different models.\n",
    "1. Simpel CNN:\n",
    "   - 3 convolutional layers: They get images with 3 channels and each one extract different amount of characteristics depending on the quantity of neurons. Moreover, they use a padding of 1.\n",
    "      - First one: Extracts 64 characteristics with 3x3 filters. Them applies ReLU and max pooling to reduce a half the resolution.\n",
    "      - Second one: Increases to 128 filters and ReLU and max pooling again.\n",
    "      - Third one: Increases to 256 filters and ReLU and max pooling again.\n",
    "   - 1 fully connected layer: After the image has been reduced and "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "\n",
    "#In case the gpu is accessible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Create the model and use the selected\n",
    "model_cnn = cnn.CNN(num_classes=2)\n",
    "model_cnn.to(device)\n",
    "\n",
    "#Define the optimizer and the loss function\n",
    "optimizer = torch.optim.Adam(model_cnn.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#Use tensorflow to see the loss functions plots and metrics\n",
    "writer = SummaryWriter()\n",
    "\n",
    "epochs = 100\n",
    "cnn.train_model(train_loader, test_loader, model_cnn, optimizer, criterion, epochs, device, writer, checkpoint_path = \"checkpoints/checkpointCNN.pth\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Evaluación en el conjunto de test\n",
    "model_cnn.eval()\n",
    "all_probs_cnn = []\n",
    "all_labels_cnn = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Obtener probabilidades de la clase positiva (galaxia)\n",
    "        outputs = model_cnn(images)\n",
    "        probs = torch.softmax(outputs, dim=1)[:, 1]  # Probabilidad de clase 1 (galaxia)\n",
    "\n",
    "        all_probs_cnn.extend(probs.cpu().numpy())  # Convertir a numpy\n",
    "        all_labels_cnn.extend(labels.cpu().numpy())  # Guardar etiquetas verdaderas\n",
    "\n",
    "# Calcular AUC-ROC\n",
    "auc_score = roc_auc_score(all_labels_cnn, all_probs_cnn)\n",
    "print(f\"AUC-ROC: {auc_score:.4f}\")\n",
    "\n",
    "# Obtener FPR y TPR para la curva ROC\n",
    "fpr_cnn, tpr_cnn, _ = roc_curve(all_labels_cnn, all_probs_cnn)\n",
    "\n",
    "# Graficar la curva ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_cnn, tpr_cnn, label=f'AUC = {auc_score:.4f}', color='#69b3fb')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Línea diagonal\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curve Images CNN')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import cnn_se\n",
    "\n",
    "#Create the model and use the selected\n",
    "model_cnn_se = cnn_se.CNNWithSE(num_classes=2)\n",
    "model_cnn_se.to(device)\n",
    "\n",
    "#Define the optimizer and the loss function\n",
    "optimizer = torch.optim.Adam(model_cnn_se.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#Use tensorflow to see the loss functions plots and metrics\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "cnn_se.train_model(train_loader, test_loader, model_cnn_se, optimizer, criterion, epochs, device, writer, checkpoint_path = \"checkpoints/checkpointCNN_SE.pth\")\n",
    "\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "#Evaluation in the test set\n",
    "model_cnn_se.eval()\n",
    "all_probs_cnn_se = []\n",
    "all_labels_cnn_se = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        #Get the probabilities for the positive class (galaxy)\n",
    "        outputs = model_cnn_se(images)\n",
    "        probs = torch.softmax(outputs, dim=1)[:, 1]  #Probability for the class 1 (galaxy)\n",
    "\n",
    "        all_probs_cnn_se.extend(probs.cpu().numpy())  #Transform it to numpy\n",
    "        all_labels_cnn_se.extend(labels.cpu().numpy())  #Save the labels\n",
    "\n",
    "#Calculate AUC-ROC curve with the obtained probabilities and labels\n",
    "auc_score = roc_auc_score(all_labels_cnn_se, all_probs_cnn_se)\n",
    "print(f\"AUC-ROC: {auc_score:.4f}\")\n",
    "\n",
    "#Obtain FPR y TPR for the ROC curve\n",
    "fpr_cnn_se, tpr_cnn_se, _ = roc_curve(all_labels_cnn_se, all_probs_cnn_se)\n",
    "\n",
    "#Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_cnn_se, tpr_cnn_se, label=f'AUC = {auc_score:.4f}', color='#0080ff')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  #Diagonal line for \"random\" classification\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curve Images CNN + SE block')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After comparing both, we can see that the SE blocks improve slightly the performance. The blue one is the simple CNN while the pink CNN + SE blocks.\n",
    "\n",
    "![](notebookImages/original.png)\n",
    "\n",
    "\n",
    "If we smooth the result we can see that the learning curve has the expected shape.\n",
    "\n",
    "![](notebookImages/smoothed.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to try to improve the results by using a grid and trying differents hyperparameters to get the best ones. For that I used the next code (it needs a lot of time). **READ THE NEXT MARKDOWN BEFORE EXECUTING IT** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimize_hyperparameters import optimize_hyperparameters\n",
    "\n",
    "best_params = optimize_hyperparameters(train_dataset, validation_dataset, device, checkpoint_path = \"checkpoints/checkpoint_hyper_optimization\")\n",
    "print(\"Best hyperparameters found:\", best_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the previous cell, you will obtain the best hyperparameters. If you have already executed it, you can safely run the next cell, as it will assign the corresponding optimal values to the variables. However, if you do not have time to run it (in my case, it took three and a half hours), you can run the next cell instead, where I assign the values I obtained.\n",
    "\n",
    "As shown in the following picture, these were the values I got after running the first cell. Since I reset the kernel, I cannot assign them directly, so I need to do it manually. Nevertheless, I am including an image to show that these were  the obtained results after running the code.\n",
    "\n",
    "![](notebookImages/hyper_opt.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_opt = best_params[\"batch_size\"]\n",
    "epochs_opt = best_params[\"epochs\"]\n",
    "lr_opt = best_params[\"lr\"]\n",
    "\n",
    "print(f\"Best batch size: {batch_size_opt}\")\n",
    "print(f\"Best number of epochs: {epochs_opt}\")\n",
    "print(f\"Best learning rate: {lr_opt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_opt = 128\n",
    "epochs_opt = 10\n",
    "lr_opt = 0.0001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the optimized hyperparameters, we will use them to get a better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import cnn_se\n",
    "\n",
    "train_loader_opt = DataLoader(train_dataset, batch_size=batch_size_opt, shuffle=True)\n",
    "test_loader_opt = DataLoader(test_dataset, batch_size=batch_size_opt, shuffle=False)\n",
    "\n",
    "#Create the model and use the selected\n",
    "model_cnn_se_opt = cnn_se.CNNWithSE(num_classes=2)\n",
    "model_cnn_se_opt.to(device)\n",
    "\n",
    "#Define the optimizer and the loss function\n",
    "optimizer = torch.optim.Adam(model_cnn_se_opt.parameters(), lr=lr_opt)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#Use tensorflow to see the loss functions plots and metrics\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "cnn_se.train_model(train_loader_opt, test_loader_opt, model_cnn_se_opt, optimizer, criterion, epochs_opt, device, writer, checkpoint_path = \"checkpoints/checkpointCNN_SE_optimized.pth\")\n",
    "\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación en el conjunto de test\n",
    "model_cnn_se_opt.eval()\n",
    "all_probs_opt = [] \n",
    "all_labels_opt = []  # <- Aquí está la variable correcta\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Obtener probabilidades de la clase positiva (galaxia)\n",
    "        outputs = model_cnn_se_opt(images)\n",
    "        probs = torch.softmax(outputs, dim=1)[:, 1]  # Probabilidad de clase 1 (galaxia)\n",
    "\n",
    "        all_probs_opt.extend(probs.cpu().numpy())  # Guardar probabilidades\n",
    "        all_labels_opt.extend(labels.cpu().numpy())  # Guardar etiquetas verdaderas\n",
    "\n",
    "# Calcular AUC-ROC\n",
    "auc_score = roc_auc_score(all_labels_opt, all_probs_opt)\n",
    "print(f\"AUC-ROC: {auc_score:.4f}\")\n",
    "\n",
    "# Obtener FPR y TPR para la curva ROC\n",
    "fpr_opt, tpr_opt, _ = roc_curve(all_labels_opt, all_probs_opt)\n",
    "\n",
    "# Graficar la curva ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_opt, tpr_opt, label=f'AUC = {auc_score:.4f}', color='#2f669c')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Línea diagonal\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curve Images CNN + SE block optimized')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Diccionario para almacenar FPR, TPR y AUC de cada modelo\n",
    "roc_data = {}\n",
    "\n",
    "# Lista de modelos y sus nombres\n",
    "model_names = [\"CNN model\", \"CNN + SE model\", \"CNN + SE optimized model\"]\n",
    "all_probs_list = [all_probs_cnn, all_probs_cnn_se, all_probs_opt]  # Reemplazar con tus listas\n",
    "all_labels_list = [all_labels_cnn, all_labels_cnn_se, all_labels_opt]  # Reemplazar con tus listas\n",
    "colors = [\"#69b3fb\", \"#0080ff\", \"#2f669c\"]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i, (probs, labels, name, color) in enumerate(zip(all_probs_list, all_labels_list, model_names, colors)):\n",
    "    # Calcular FPR, TPR y AUC\n",
    "    fpr, tpr, _ = roc_curve(labels, probs)\n",
    "    auc_score = roc_auc_score(labels, probs)\n",
    "\n",
    "    # Guardar datos en el diccionario (opcional)\n",
    "    roc_data[name] = {'fpr': fpr, 'tpr': tpr, 'auc': auc_score}\n",
    "\n",
    "    # Graficar la curva ROC de cada modelo\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.4f})', color=color)\n",
    "\n",
    "# Línea diagonal de referencia\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "\n",
    "# Configurar gráfico\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('All ROC curves comparison (Images)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdataenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
