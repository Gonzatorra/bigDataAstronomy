{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective of the project üöÄ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My objective is to create a machine learning model for object classification and then, try and compare it with a convolutional neural network (CNN) for images. For that, I will use Spark MLlib to train and evaluate the model. Secondly, create the CNN and compare both ü™ê\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source of the data is: https://skyserver.sdss.org/CasJobs/\n",
    "It is necessary to register and login a user to download the data you want. Then, you have to make a query specifying:\n",
    "- Amount of rows\n",
    "- Columns\n",
    "- Where to keep the csv\n",
    "- The database\n",
    "\n",
    "As I want to get as much as possible data, I will not a maximum of rows.\n",
    "\n",
    "I also add a \"where\" so I can get only data from planets, galaxies and stars:\n",
    "- type = 3: Galaxies\n",
    "- type = 6: Stars\n",
    "\n",
    "and I downloaded the dataset to start working with it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/home/haizeagonzalez/myproject/bigDataAstronomy/notebookImages/img1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns we have are:\n",
    "- objID: Unique identifier of the object ‚Üí TYPE bigInt\n",
    "- ra: Right ascension ‚Üí TYPE float\n",
    "- dec: Declination ‚Üí TYPE float\n",
    "- petroRad: Petrosian radius, used to know the size of galaxies in astronomical pictures. It is the amount of light that a galaxy emits in a sepecific radius. Very used because it is independent of the distance and brightness. We use different photometric filters:\n",
    "    - petroRad_u: Near-ultraviolet\n",
    "    - petroRad_g: Blue-Green\n",
    "    - petroRad_r: Red\n",
    "    - petroRad_i: Near-infrared\n",
    "    - petroRad_z: Deeper infrared\n",
    " ‚Üí TYPE: Real\n",
    "\n",
    "- modelMag: Brightness measure adjusted to a galaxy model. Usual for galaxies. Also for all filters (u, g, r, i and z) ‚Üí TYPE Real\n",
    "- psfMag: Brightness measure based on the point source light profile. Usual for stars. Also for all filters (u, g, r, i and z) ‚Üí TYPE Real\n",
    "- u_g: (modelMag_u - modelMag_g)\n",
    "- g_r: (modelMag_g - modelMag_r)\n",
    "- r_i: (modelMag_r - modelMag_i)\n",
    "- i_z: (modelMag_i - modelMag_z)\n",
    "- fracDeV: The amount of brightness that the object has in the De Vaucouleurs profile. Also for all filters (u, g, r, i and z) ‚Üí TYPE Real\n",
    "- flags: Bit comination that explains different characteristics of the object. If we convert it to binary and check SDSS documentarion, we get a meaning for each bit ‚Üí TYPE bigInt\n",
    "- clean: Indicator that tell us if the object was cleaned ‚Üí TYPE int\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PetroRad:\n",
    "- Stars: Small and constant in all filters.\n",
    "- Galaxies: Bigger and variates depending on the wavelengths.\n",
    "\n",
    "ModelMag and psfMag:\n",
    "- In the red filter:\n",
    "    - Stars: modelMag_r ‚âà psfMag_r\n",
    "    - Galaxies: modelMag_r > psfMag_r\n",
    "- In other filers:\n",
    "    - Galaxies are usuarlly more red  (modelMag_g - modelMag_r is big).\n",
    "    - Stars has different colors depending on their type.\n",
    "\n",
    "fracDeV:\n",
    "- Stars: fracDeV ‚âà 0.\n",
    "- Galaxies: fracDeV ‚âà 1 (eliptic) or fracDeV < 1 (espiral).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a spark sesion in case there is no one or get if there exists: \"getOrCreate\". I also decided to create a log in case there is any error during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/27 17:25:41 WARN Utils: Your hostname, SSMRS3-04969600 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/03/27 17:25:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Exception in thread \"main\" java.lang.ExceptionInInitializerError\n",
      "\tat org.apache.spark.unsafe.array.ByteArrayMethods.<clinit>(ByteArrayMethods.java:54)\n",
      "\tat org.apache.spark.internal.config.package$.<init>(package.scala:1149)\n",
      "\tat org.apache.spark.internal.config.package$.<clinit>(package.scala)\n",
      "\tat org.apache.spark.deploy.SparkSubmitArguments.$anonfun$loadEnvironmentArguments$3(SparkSubmitArguments.scala:157)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.deploy.SparkSubmitArguments.loadEnvironmentArguments(SparkSubmitArguments.scala:157)\n",
      "\tat org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:115)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1026)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1026)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:85)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: java.lang.IllegalStateException: java.lang.NoSuchMethodException: java.nio.DirectByteBuffer.<init>(long,int)\n",
      "\tat org.apache.spark.unsafe.Platform.<clinit>(Platform.java:113)\n",
      "\t... 13 more\n",
      "Caused by: java.lang.NoSuchMethodException: java.nio.DirectByteBuffer.<init>(long,int)\n",
      "\tat java.base/java.lang.Class.getConstructor0(Class.java:3761)\n",
      "\tat java.base/java.lang.Class.getDeclaredConstructor(Class.java:2930)\n",
      "\tat org.apache.spark.unsafe.Platform.<clinit>(Platform.java:71)\n",
      "\t... 13 more\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      3\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbigDataAstronomyProject\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.executor.memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m8g\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m4g\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.executor.cores\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.executor.instances\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m spark.sparkContext.setLogLevel(\u001b[33m\"\u001b[39m\u001b[33mERROR\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bigData/bigDataVenv/lib/python3.12/site-packages/pyspark/sql/session.py:497\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    500\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bigData/bigDataVenv/lib/python3.12/site-packages/pyspark/context.py:515\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bigData/bigDataVenv/lib/python3.12/site-packages/pyspark/context.py:201\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    203\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    204\u001b[39m         master,\n\u001b[32m    205\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    215\u001b[39m         memory_profiler_cls,\n\u001b[32m    216\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bigData/bigDataVenv/lib/python3.12/site-packages/pyspark/context.py:436\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bigData/bigDataVenv/lib/python3.12/site-packages/pyspark/java_gateway.py:107\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    104\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    108\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    109\u001b[39m         message_parameters={},\n\u001b[32m    110\u001b[39m     )\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[32m    113\u001b[39m     gateway_port = read_int(info)\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"bigDataAstronomyProject\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to read de csv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "df = spark.read.csv(config.DATA_PATH, header=True)\n",
    "print(df.count())\n",
    "#df2 = spark.read.csv(config.DATA_PATH2, header=True)\n",
    "#print(df2.count())\n",
    "\n",
    "#df = df.union(df2)\n",
    "#print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to check if the data is correctly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema and the chacacteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all columns are string, we need to convert them into their type. For that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mdf\u001b[49m.withColumn(\u001b[33m\"\u001b[39m\u001b[33mobjID\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mobjID\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mlong\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m      4\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mra\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mra\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m      5\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mdec\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mdec\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m      6\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mpetroRad_u\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mpetroRad_u\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m      7\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mpetroRad_g\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mpetroRad_g\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m      8\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mpetroRad_r\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mpetroRad_r\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m      9\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mpetroRad_i\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mpetroRad_i\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     10\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mpetroRad_z\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mpetroRad_z\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     11\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mmodelMag_u\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mmodelMag_u\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     12\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mmodelMag_g\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mmodelMag_g\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     13\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mmodelMag_r\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mmodelMag_r\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     14\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mmodelMag_i\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mmodelMag_i\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     15\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mmodelMag_z\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mmodelMag_z\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     16\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mpsfMag_u\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mpsfMag_u\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     17\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mpsfMag_g\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mpsfMag_g\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     18\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mpsfMag_r\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mpsfMag_r\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     19\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mpsfMag_i\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mpsfMag_i\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     20\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mpsfMag_z\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mpsfMag_z\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     21\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mu_g\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mu_g\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     22\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mg_r\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mg_r\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     23\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mr_i\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mr_i\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     24\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mi_z\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mi_z\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     25\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mfracDeV_u\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mfracDeV_u\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     26\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mfracDeV_g\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mfracDeV_g\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     27\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mfracDeV_r\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mfracDeV_r\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     28\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mfracDeV_i\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mfracDeV_i\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     29\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mfracDeV_z\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mfracDeV_z\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     30\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mflags\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mflags\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mlong\u001b[39m\u001b[33m\"\u001b[39m)) \\\n\u001b[32m     31\u001b[39m        .withColumn(\u001b[33m\"\u001b[39m\u001b[33mclean\u001b[39m\u001b[33m\"\u001b[39m, col(\u001b[33m\"\u001b[39m\u001b[33mclean\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     33\u001b[39m df.printSchema()\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"objID\", col(\"objID\").cast(\"long\")) \\\n",
    "       .withColumn(\"ra\", col(\"ra\").cast(\"float\")) \\\n",
    "       .withColumn(\"dec\", col(\"dec\").cast(\"float\")) \\\n",
    "       .withColumn(\"petroRad_u\", col(\"petroRad_u\").cast(\"float\")) \\\n",
    "       .withColumn(\"petroRad_g\", col(\"petroRad_g\").cast(\"float\")) \\\n",
    "       .withColumn(\"petroRad_r\", col(\"petroRad_r\").cast(\"float\")) \\\n",
    "       .withColumn(\"petroRad_i\", col(\"petroRad_i\").cast(\"float\")) \\\n",
    "       .withColumn(\"petroRad_z\", col(\"petroRad_z\").cast(\"float\")) \\\n",
    "       .withColumn(\"modelMag_u\", col(\"modelMag_u\").cast(\"float\")) \\\n",
    "       .withColumn(\"modelMag_g\", col(\"modelMag_g\").cast(\"float\")) \\\n",
    "       .withColumn(\"modelMag_r\", col(\"modelMag_r\").cast(\"float\")) \\\n",
    "       .withColumn(\"modelMag_i\", col(\"modelMag_i\").cast(\"float\")) \\\n",
    "       .withColumn(\"modelMag_z\", col(\"modelMag_z\").cast(\"float\")) \\\n",
    "       .withColumn(\"psfMag_u\", col(\"psfMag_u\").cast(\"float\")) \\\n",
    "       .withColumn(\"psfMag_g\", col(\"psfMag_g\").cast(\"float\")) \\\n",
    "       .withColumn(\"psfMag_r\", col(\"psfMag_r\").cast(\"float\")) \\\n",
    "       .withColumn(\"psfMag_i\", col(\"psfMag_i\").cast(\"float\")) \\\n",
    "       .withColumn(\"psfMag_z\", col(\"psfMag_z\").cast(\"float\")) \\\n",
    "       .withColumn(\"u_g\", col(\"u_g\").cast(\"float\")) \\\n",
    "       .withColumn(\"g_r\", col(\"g_r\").cast(\"float\")) \\\n",
    "       .withColumn(\"r_i\", col(\"r_i\").cast(\"float\")) \\\n",
    "       .withColumn(\"i_z\", col(\"i_z\").cast(\"float\")) \\\n",
    "       .withColumn(\"fracDeV_u\", col(\"fracDeV_u\").cast(\"float\")) \\\n",
    "       .withColumn(\"fracDeV_g\", col(\"fracDeV_g\").cast(\"float\")) \\\n",
    "       .withColumn(\"fracDeV_r\", col(\"fracDeV_r\").cast(\"float\")) \\\n",
    "       .withColumn(\"fracDeV_i\", col(\"fracDeV_i\").cast(\"float\")) \\\n",
    "       .withColumn(\"fracDeV_z\", col(\"fracDeV_z\").cast(\"float\")) \\\n",
    "       .withColumn(\"flags\", col(\"flags\").cast(\"long\")) \\\n",
    "       .withColumn(\"clean\", col(\"clean\").cast(\"int\"))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the structure, we are going to explore and clean the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, the data is cleaned because we get it from CasJobs and we apply clear filter to get good data. \n",
    "However, we are going to check whether there is any null value and the amount of galaxies and stars.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "number_stars = df.filter(df.type == 6).count()\n",
    "number_galaxies = df.filter(df.type == 3).count()\n",
    "\n",
    "#Convert the data into a dataframe to make the plot\n",
    "data = pd.DataFrame({\n",
    "    \"type\": [\"Stars\", \"Galaxies\"],\n",
    "    \"count\": [number_stars, number_galaxies]\n",
    "})\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(data[\"type\"], data[\"count\"], color=[\"#ff9300\", \"#42c9c9\"])\n",
    "plt.xlabel(\"Object type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Total of Stars and Galaxies\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is no null values and the amount of galaxies are less than the amount of stars, which make sense."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to change \"type\" label. As it is a binary classification, we will update stars to 0 and galaxies to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"type\", when(col(\"type\") == 3,1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stars = df.filter(df[\"type\"] == 0)\n",
    "df_galaxies = df.filter(df[\"type\"] == 1)\n",
    "\n",
    "#Print to know the conversion is correctly done\n",
    "df_stars.show(5)\n",
    "df_galaxies.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the supervised machine model we won't use objID, ra, dec, flags and clean columns, we are going to remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_model = df.select(\"type\", \"petroRad_u\", \"petroRad_g\", \"petroRad_r\", \"petroRad_i\", \"petroRad_z\",\n",
    "                        \"modelMag_u\", \"modelMag_g\", \"modelMag_r\", \"modelMag_i\", \"modelMag_z\",\n",
    "                        \"psfMag_u\", \"psfMag_g\", \"psfMag_r\", \"psfMag_i\", \"psfMag_z\",\n",
    "                        \"u_g\", \"g_r\", \"r_i\", \"i_z\",\n",
    "                        \"fracDeV_u\", \"fracDeV_g\", \"fracDeV_r\", \"fracDeV_i\", \"fracDeV_z\")\n",
    "\n",
    "df_ml_model.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our objective is to create a machine learning model, we need to convert the data in a correct format: Vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "features = df_ml_model.columns[1:] #We don't get the type beacuse is the result we want to get.\n",
    "assembler = VectorAssembler(inputCols = features, outputCol = \"features\") #Convert features list as a vector\n",
    "df_ml_model = assembler.transform(df_ml_model) #Apply transformation\n",
    "df_ml_model = df_ml_model.select(\"features\", \"type\")\n",
    "\n",
    "df_ml_model.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to divide the dataset into train and test, so we can get the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_ml_model.randomSplit([0.8, 0.2], seed = 132)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to try different models to check which is the best for our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, LinearSVC\n",
    "\n",
    "#Define different models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(labelCol=\"type\", featuresCol=\"features\"),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(labelCol=\"type\", featuresCol=\"features\"),\n",
    "    \"Random Forest\": RandomForestClassifier(labelCol=\"type\", featuresCol=\"features\", numTrees=5),\n",
    "    \"Gradient Boosted Trees\": GBTClassifier(labelCol=\"type\", featuresCol=\"features\"),\n",
    "    \"Linear SVM\": LinearSVC(labelCol=\"type\", featuresCol=\"features\")\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to calculate the **AUC evaluator** for model and look how well they are doing. I decided to use AUC because it is a good technique to use for binary classification, especially when classes are unbalanced.\n",
    "\n",
    "Remember that the ROC curve gives a visual representation between the true prositive rate and false positive rate at different thresholds. So, it tell us how well the model can detect true positive and avoid false positives. Moreover, AUC is one scalar value from 0 to  that tell how the model performs globally. AUC represents the area beneath the curve.\n",
    "- AUC = 0.5 ‚≠¢ Random Predictions\n",
    "- AUC > 0.5 ‚≠¢ Increasingly good predictions\n",
    "- AUC = 1 ‚≠¢ Perfect predictions\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"/home/haizeagonzalez/myproject/bigDataAstronomy/notebookImages/img2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"type\", metricName=\"areaUnderROC\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    model_trained = model.fit(train_data)\n",
    "    predictions = model_trained.transform(test_data)\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    print(f\"{name}: AUC = {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#Create the figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "#Different colors for the different models\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "#For each model compute the AUC\n",
    "for idx, (name, model) in enumerate(models.items()):\n",
    "    model_trained = model.fit(train_data)\n",
    "    predictions = model_trained.transform(test_data) #Predicted class (0 or 1)\n",
    "\n",
    "    #Check if the model has \"probability\" column because some of them (SVG for example) do not\n",
    "    if \"probability\" in predictions.columns:\n",
    "        #Get the whole probability vector, convert it into rdd and select the probability for the positive (real) prediction\n",
    "        prob_positives = predictions.select(\"probability\").rdd.map(lambda row: row[0][1]).collect()\n",
    "        #Get the label for each prediction\n",
    "        true_labels = predictions.select(\"type\").rdd.map(lambda row: row[0]).collect()\n",
    "\n",
    "        #Calculate ROC curve\n",
    "        fpr, tpr, _ = roc_curve(true_labels, prob_positives)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        #Plot it\n",
    "        plt.plot(fpr, tpr, color=colors[idx % len(colors)], lw=2, label=f'{name} (AUC = {roc_auc:.4f})')\n",
    "    else:\n",
    "        print(f\"The model {name} does not have 'probability' column so it will not be in the graphic.\")\n",
    "\n",
    "#Different parameters to personalize the plot\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  #Diagonal line of \"random\"\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the best AUC is the one for logistic regression so we will use that model in order to make our predictions. \n",
    "\n",
    "Then, as our model has a very high AUC, we need to ensure that it is not overfitted. A high AUC does not necessarily mean good generalization; it could indicate that the model memorized the training data.\n",
    "\n",
    "To ensure about this, we will compute the confusion matrix for training and test sets. Comparing both we will know if the model is performing well or not with unseen data. We will use plots to visualize the matrices in order to make an easier and more interpetable comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"type\", featuresCol=\"features\")\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "predictions_log_train = lr_model.transform(train_data)\n",
    "predictions_log_test = lr_model.transform(test_data)\n",
    "\n",
    "auc_train = evaluator.evaluate(predictions_log_train)\n",
    "auc_test = evaluator.evaluate(predictions_log_test)\n",
    "\n",
    "print(f\"Logistic Regression (Train): AUC = {auc_train:.4f}\\n\")\n",
    "print(f\"Logistic Regression (Test): AUC = {auc_test:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the difference is almost non-existent which is good because this suggests that the model is generalizing correctly, data for train and test set are representative of the data. However, as it is a very high AUC, we can verify the confusion matrix in order to know if the model is correctly classifying. To make this, we will use the plot of the confusion matrix to visualize to get a conclusion in a easier and more interpetable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "#Divide true positives, true negatives, false positives and false negatives\n",
    "##Train\n",
    "true_galaxies_train = predictions_log_train.filter((F.col(\"type\") == 1) & (F.col(\"prediction\") == 1)).count()\n",
    "true_stars_train = predictions_log_train.filter((F.col(\"type\") == 0) & (F.col(\"prediction\") == 0)).count()\n",
    "false_stars_train = predictions_log_train.filter((F.col(\"type\") == 0) & (F.col(\"prediction\") == 1)).count()\n",
    "false_galaxies_train = predictions_log_train.filter((F.col(\"type\") == 1) & (F.col(\"prediction\") == 0)).count()\n",
    "\n",
    "#Creating the confusion matrix\n",
    "conf_matrix_values_train = [[true_stars_train, false_stars_train], [false_galaxies_train, true_galaxies_train]]\n",
    "\n",
    "\n",
    "##Test\n",
    "true_galaxies_test = predictions_log_test.filter((F.col(\"type\") == 1) & (F.col(\"prediction\") == 1)).count()\n",
    "true_stars_test = predictions_log_test.filter((F.col(\"type\") == 0) & (F.col(\"prediction\") == 0)).count()\n",
    "false_stars_test = predictions_log_test.filter((F.col(\"type\") == 0) & (F.col(\"prediction\") == 1)).count()\n",
    "false_galaxies_test = predictions_log_test.filter((F.col(\"type\") == 1) & (F.col(\"prediction\") == 0)).count()\n",
    "\n",
    "#Creating the confusion matrix\n",
    "conf_matrix_values_test = [[true_stars_test, false_stars_test], [false_galaxies_test, true_galaxies_test]]\n",
    "\n",
    "\n",
    "#Plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))  #Create two plots in 1 line\n",
    "\n",
    "#Train confusion matrix\n",
    "ax1 = sns.heatmap(np.array(conf_matrix_values_train), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                   xticklabels=[\"Stars prediction\", \"Galaxies prediction\"],\n",
    "                   yticklabels=[\"Real stars\", \"Real galaxies\"], ax=axes[0]) #Put in the axe 0, the left one\n",
    "ax1.set_xlabel(\"Prediction\")\n",
    "ax1.set_ylabel(\"Real value\")\n",
    "ax1.set_title(\"Confusion Matrix - Train\")\n",
    "\n",
    "#Test confusion matrix\n",
    "ax2 = sns.heatmap(np.array(conf_matrix_values_test), annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                   xticklabels=[\"Stars prediction\", \"Galaxies prediction\"],\n",
    "                   yticklabels=[\"Real stars\", \"Real galaxies\"], ax=axes[1]) #Put in the axe 0, the right one\n",
    "ax2.set_xlabel(\"Prediction\")\n",
    "ax2.set_ylabel(\"Real value\")\n",
    "ax2.set_title(\"Confusion Matrix - Test\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect if there is or not overfitting, we need to compare the error rate in train and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train\n",
    "false_stars_train = conf_matrix_values_train[1][0]\n",
    "true_galaxies_train = conf_matrix_values_train[1][1]\n",
    "\n",
    "false_stars_train_rate = ((false_stars_train)/(false_stars_train + true_galaxies_train))*100\n",
    "false_stars_train_rate = round(false_stars_train_rate,2)\n",
    "\n",
    "\n",
    "##Test\n",
    "false_stars_test = conf_matrix_values_test[1][0]\n",
    "true_galaxies_test = conf_matrix_values_test[1][1]\n",
    "\n",
    "false_stars_test_rate = ((false_stars_test)/(false_stars_test + true_galaxies_test))*100\n",
    "false_stars_test_rate = round(false_stars_test_rate,2)\n",
    "\n",
    "\n",
    "#PLOTS\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))  #Create two plots in 1 line\n",
    "\n",
    "labels_train = [\"Correct Predictions\", \"False Stars\"]\n",
    "sizes_train = [100 - false_stars_train_rate, false_stars_train_rate]\n",
    "colors_train = [\"#ff9300\", \"#9b9c9c\"]\n",
    "axes[0].pie(sizes_train, labels=labels_train, colors=colors_train, autopct='%1.1f%%', startangle=140)\n",
    "axes[0].set_title(f\"Train - False Stars Rate: {false_stars_train_rate}%\")\n",
    "\n",
    "# Pie Chart para Test\n",
    "labels_test = [\"Correct Predictions\", \"False Stars\"]\n",
    "sizes_test = [100 - false_stars_test_rate, false_stars_test_rate]\n",
    "colors_test = [\"#ff9300\", \"#9b9c9c\"]\n",
    "axes[1].pie(sizes_test, labels=labels_test, colors=colors_test, autopct='%1.1f%%', startangle=140)\n",
    "axes[1].set_title(f\"Test - False Stars Rate: {false_stars_test_rate}%\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train\n",
    "false_galaxies_train = conf_matrix_values_train[0][1]\n",
    "true_stars_train = conf_matrix_values_train[0][0]\n",
    "\n",
    "false_galaxies_train_rate = ((false_galaxies_train)/(false_galaxies_train + true_stars_train))*100\n",
    "false_galaxies_train_rate = round(false_galaxies_train_rate,2)\n",
    "\n",
    "##Test\n",
    "false_galaxies_test = conf_matrix_values_test[0][1]\n",
    "true_stars_test = conf_matrix_values_test[0][0]\n",
    "\n",
    "false_galaxies_test_rate = ((false_galaxies_test)/(false_galaxies_test + true_stars_test))*100\n",
    "false_galaxies_test_rate = round(false_galaxies_test_rate,2)\n",
    "\n",
    "\n",
    "#PLOTS\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))  #Create two plots in 1 line\n",
    "\n",
    "labels_train = [\"Correct Predictions\", \"False Stars\"]\n",
    "sizes_train = [100 - false_galaxies_train_rate, false_galaxies_train_rate]\n",
    "colors_train = [\"#42c9c9\", \"#9b9c9c\"]\n",
    "axes[0].pie(sizes_train, labels=labels_train, colors=colors_train, autopct='%1.1f%%', startangle=140)\n",
    "axes[0].set_title(f\"Train - False Galaxies Rate: {false_galaxies_train_rate}%\")\n",
    "\n",
    "# Pie Chart para Test\n",
    "labels_test = [\"Correct Predictions\", \"False Stars\"]\n",
    "sizes_test = [100 - false_galaxies_test_rate, false_galaxies_test_rate]\n",
    "colors_test = [\"#42c9c9\", \"#9b9c9c\"]\n",
    "axes[1].pie(sizes_test, labels=labels_test, colors=colors_test, autopct='%1.1f%%', startangle=140)\n",
    "axes[1].set_title(f\"Test - False Galaxies Rate: {false_galaxies_test_rate}%\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As both rates (train and test) are very similar, the model is not overfitting so it is generalizing correctly the data. However, As the AUC value is very big, we can use **regularization** technique to make a more robust model. Regularization is intended to prevent overfitting by penalizing large coefficients and forcing the model to learn simpler, more generalizable patterns. There are different types of regularization (L1, L2 (same that L1 but squared), L1+L2, dropout,...). In this case, we will use L2 regularization because it is the most estable one and because all the selected variables are relevant. For that, it is necessary to add *regParam* to the Logistic Regression. The selected value 0.1 is because it is a simple model, it has not many variables (just 24)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_with_reg = LogisticRegression(labelCol=\"type\", featuresCol=\"features\", regParam=0.1)\n",
    "model_with_reg = lr_with_reg.fit(train_data)\n",
    "\n",
    "#Evaluate the model with regularization\n",
    "predictions_with_reg = model_with_reg.transform(test_data)\n",
    "auc_with_reg = evaluator.evaluate(predictions_with_reg)\n",
    "print(f\"Logistic Regression (Regularization): AUC = {auc_with_reg:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the AUC has decreased from 0.9919 to 0.9066 (in test). This will be good for the model to learn more generalizable patterns and not to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I need also de images, I have downloaded from https://skyserver.sdss.org/dr18, specifying with a request:\n",
    "- the location of the object (with right ascension (RA) and declination (dec))\n",
    "- the zoom of the picture (scale)\n",
    "- the dimmensions of the photo (with and height)\n",
    "\n",
    "These images will be saved into a \"images\" folder splitted into train and test and each one splitted as galaxies or stars. In a visual way would be:\n",
    "|\n",
    "|\n",
    "images\n",
    "   |\n",
    "   |_ train\n",
    "       |\n",
    "       |_ star\n",
    "       |_ galaxy\n",
    "    |\n",
    "    |_ test\n",
    "       |\n",
    "       |_ star\n",
    "       |_ galaxy\n",
    "   \n",
    "This python file is out of this notebook not to collapse spark, its name is \"download_images.py\".\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First it is necessary to know the amount of channels we are going to work with. For that, we are going to take one photo and verify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import config\n",
    "\n",
    "image_path = config.IMAGES_PATH + \"/train/star\"\n",
    "\n",
    "#Get all the images\n",
    "images = [f for f in os.listdir(image_path) if f.endswith((\".jpg\"))]\n",
    "\n",
    "image_path = os.path.join(image_path, images[0]) #Get the first photo\n",
    "\n",
    "image = cv2.imread(image_path) #Read the first photo\n",
    "\n",
    "if len(image.shape) == 2:\n",
    "    print(\"Image with grayscale.\")\n",
    "elif len(image.shape) == 3 and image.shape[2] == 3:\n",
    "    print(\"Image with 3 channels (RGB).\")\n",
    "elif len(image.shape) == 3 and image.shape[2] == 4:\n",
    "    print(\"Image with 4 channels (RGBA).\") #RGB + Alpha (opacity of the image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our images have 3 channels RGB. Now, we are going to normalize the pixels values  because we are going to work with a neural network and usually this improves the model performance and facilitate the process of learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, regexp_extract\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "import numpy as np\n",
    "import io\n",
    "import config\n",
    "from PIL import Image\n",
    "\n",
    "def normalize_and_resize_image(image_bytes):\n",
    "    # Abrir la imagen desde bytes\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    \n",
    "    # Redimensionar la imagen a 64x64\n",
    "    image = image.resize((64, 64))\n",
    "    \n",
    "    # Convertir la imagen en un array de numpy\n",
    "    image_array = np.array(image)\n",
    "    \n",
    "    # Normalizar a valores entre 0 y 1\n",
    "    image_array = image_array.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Asegurarse de que la imagen tiene 3 canales (RGB)\n",
    "    if image_array.shape[-1] != 3:\n",
    "        raise ValueError(f\"Imagen tiene {image_array.shape[-1]} canales, debe tener 3 (RGB).\")\n",
    "    \n",
    "    # Aplanar la imagen para el procesamiento en PySpark\n",
    "    return image_array.flatten().tolist()\n",
    "\n",
    "#UDF(personalized functions) for PySpark\n",
    "normalize_image_udf = udf(normalize_and_resize_image, ArrayType(FloatType()))\n",
    "\n",
    "#Load all the images\n",
    "df = spark.read.format(\"binaryFile\").load(\"file:///home/haizeagonzalez/bigData/images/*/*/*.jpg\") \\\n",
    "    .filter(~col(\"path\").contains(\"Identifier\"))\n",
    "\n",
    "#Extract the types and classes\n",
    "df = df.withColumn(\"set_type\", regexp_extract(col(\"path\"), r\"\\/(train|test)\\/\", 1))  #\"train\" or \"test\"\n",
    "df = df.withColumn(\"class_label\", regexp_extract(col(\"path\"), r\"\\/(star|galaxy)\\/\", 1))  #\"star\" or \"galaxy\"\n",
    "\n",
    "#Normalize the images\n",
    "df = df.withColumn(\"normalized_image\", normalize_image_udf(df[\"content\"]))\n",
    "\n",
    "\n",
    "\n",
    "# Guardar en formato Parquet\n",
    "df.select(\"path\", \"set_type\", \"class_label\", \"normalized_image\").write.parquet(config.ORIGINAL_PATH + \"/processed_images\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funci√≥n normalize_and_resize_image redimensiona las im√°genes a 64x64 p√≠xeles y normaliza los valores de los p√≠xeles para que est√©n entre 0 y 1. Este proceso se aplica a cada imagen en el DataFrame de PySpark mediante el UDF (normalize_image_udf).\n",
    "\n",
    "Despu√©s de aplicar este UDF a las im√°genes, las im√°genes son transformadas a un formato normalizado y redimensionado, y se guardan en la columna normalized_image.\n",
    "\n",
    "Guardar en formato Parquet:\n",
    "\n",
    "El c√≥digo guarda el DataFrame que incluye la columna normalized_image (junto con las otras columnas como path, set_type, y class_label) en formato Parquet en la ubicaci√≥n especificada por config.ORIGINAL_PATH + \"/processed_images\".\n",
    "\n",
    "Si la ruta de config.ORIGINAL_PATH est√° correctamente definida, y no hay errores durante la ejecuci√≥n, entonces las im√°genes procesadas (normalizadas) deber√≠an haber sido guardadas en el directorio processed_images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"set_type\", \"class_label\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = spark.read.parquet(config.ORIGINAL_PATH + \"/processed_images\")\n",
    "processed_df.show(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.groupBy(\"class_label\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mostrar una imagen aleatoria de las normalizadas\n",
    "img = np.array(processed_df.collect()[0][\"normalized_image\"]).reshape(64, 64, 3)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = processed_df.randomSplit([0.8, 0.2], seed=132)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the processed df (Spark dataFrame) into a Pandas dataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_pd = processed_df.toPandas()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdataenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
